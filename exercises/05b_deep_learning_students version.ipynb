{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "## <span style=\"color:#4375c7\">DAI</span>\n",
    "***\n",
    "*Course materials are for educational purposes only. Nothing contained herein should be considered investment advice or an opinion regarding the suitability of any security. For more information about this course, please contact us.*\n",
    "***\n",
    "\n",
    "### Session contents:\n",
    "1. **[Deep Learning - Generative models ](#NN)**\n",
    "    - [Variational autoencoder](#VAE)\n",
    "    - [Generative adversarial network](#GAN)\n",
    "\n",
    "    \n",
    "2. **[Hands-on session](#ho)**\n",
    "***\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<br/><br/>\n",
    "\n",
    "## 2. Hands-on session <a id='ho'></a>\n",
    "\n",
    "### Exercise 1\n",
    "1. Read the day-ahead electricity data set and visualize a random set of daily auctions. Each auction contains 24 load auction prices, one for each hour of the day. Fill in the missing parts of the code marked with ___. \n",
    "\n",
    "2. Use the given structure of a GAN to generate new time series. Fill in the missing parts of the code marked with ___. Use dense layers instead of convolutional layers. Execute the code and generate new time series.\n",
    "\n",
    "\n",
    "### Exercise 2\n",
    "1.  Read the day-ahead electricity dataset and split the dataset into a test dataset and a train dataset. Do a split after four years of data. Keep leap years in mind. \n",
    "\n",
    "2. Use the given structure of a VAE to generate new time series. Fill in the missing parts of the code marked with ___. Use dense layers instead of convolutional layers. Execute the code and generate new time series.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "os.chdir('/home/jovyan/work/DAI')  # Provide the new path here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1)\n",
    "import pandas as ___\n",
    "import numpy as ___\n",
    "import matplotlib.___ as ___\n",
    "#read in the data and plot a random set\n",
    "#read in the data\n",
    "x_train = pd.DataFrame(pd.read_csv('___.csv')[1:]).values\n",
    "\n",
    "plt.plot(x_train[np.random.___(1,1000)])\n",
    "plt.xlabel(\"hours\")\n",
    "plt.ylabel(\"dayahead price\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#2)\n",
    "# Lets start with the gan\n",
    "#define your parameters\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "latent_dim = 24\n",
    "iterations = 10000\n",
    "batch_size = 100\n",
    "\n",
    "# Read in the data\n",
    "shapes = pd.read_csv('Dataset_dayahead.csv')\n",
    "\n",
    "# Convert the data to float32\n",
    "x_train = np.array(shapes).astype(np.float32)\n",
    "\n",
    "#define the generator\n",
    "generator_input = keras.Input(shape=(latent_dim, ))\n",
    "x = layers.___(24)(___)\n",
    "x = layers.___(0.3)(x)\n",
    "x = layers.___(128)(x)\n",
    "x = layers.___(0.3)(x)\n",
    "x = layers.___(256)(x)\n",
    "x = layers.___(0.3)(x)\n",
    "x = layers.___(24, activation='___')(x)\n",
    "generator_output = keras.models.Model(generator_input, generator_output)\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the discriminator\n",
    "discriminator_input = layers.Input(shape=(latent_dim, ))\n",
    "x = layers.___(256)(discriminator_input)\n",
    "x = layers.___(0.3)(x)\n",
    "x = layers.___(128)(x)\n",
    "x = layers.___(0.3)(x)\n",
    "x = layers.Dropout(0.3)(x)  #important trick for robustness\n",
    "discriminator_output = layers.___(1, activation='___')(x)\n",
    "discriminator = keras.models.Model(discriminator_input, discriminator_output)\n",
    "discriminator.summary()\n",
    "\n",
    "discriminator_optimizer = keras.optimizers.RMSprop(learning_rate=0.0004,\n",
    "                                                   clipvalue=1.0,\n",
    "                                                   decay=1e-8)\n",
    "discriminator.compile(optimizer=discriminator_optimizer,\n",
    "                      loss='___')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#GAN model: discriminator not trainable during the training of the Generator !\n",
    "discriminator.trainable = ___\n",
    "gan_input = keras.Input(shape=(latent_dim, ))\n",
    "gan_output = discriminator(generator(gan_input))\n",
    "gan = keras.models.Model(gan_input, gan_output)\n",
    "#gan.summary()\n",
    "\n",
    "gan_optimizer = keras.optimizers.RMSprop(learning_rate=0.004, clipvalue=1.0, decay=1e-8)\n",
    "gan.compile(optimizer=gan_optimizer, loss='___')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss_a, loss_d = [], []\n",
    "start = 0\n",
    "\n",
    "#train:\n",
    "for step in tqdm(range(iterations)):\n",
    "    # Draw random points from latent space\n",
    "    random_latent_vectors = np.random.normal(size=(batch_size, latent_dim))\n",
    "    # Generate artificial data points with the generator\n",
    "    generated_ts = generator.predict(___)  \n",
    "\n",
    "    # Mix real and fake data points\n",
    "    stop = start + batch_size\n",
    "    real_ts = x_train[start:stop]\n",
    "    combined_ts = np.concatenate([generated_ts, real_ts])\n",
    "    labels = np.concatenate(\n",
    "        [np.___((batch_size, 1)),\n",
    "         np.___((batch_size, 1))])\n",
    "    # Add random noise to the labels for robustness\n",
    "    labels += 0.05 * np.random.random(labels.shape)\n",
    "    # Unfreeze the discriminator weights\n",
    "    discriminator.trainable = ___\n",
    "    #train the discriminator\n",
    "    d_loss = discriminator.train_on_batch(combined_ts, labels)\n",
    "\n",
    "    #draw new random points\n",
    "    random_latent_vector = np.random.normal(size=(batch_size, latent_dim))\n",
    "    #set misleading targets\n",
    "    misleading_targets = np.zeros((batch_size, 1))\n",
    "    #freeze the discriminator weights\n",
    "    discriminator.trainable = ___\n",
    "\n",
    "    #train the generator\n",
    "    a_loss = gan.train_on_batch(random_latent_vectors,\n",
    "                                ___)  #train the generator \n",
    "\n",
    "    #go to the next batch\n",
    "    start += batch_size\n",
    "\n",
    "    if start > len(x_train) - batch_size:\n",
    "        start = 0\n",
    "    # Add losses of generator and discriminator for later plots\n",
    "    if step % 10 == 0:  # Record losses every 10 steps\n",
    "        print(f'discriminator loss at step {step}: {d_loss}')\n",
    "        print(f'adversarial loss at step {step}: {a_loss}')\n",
    "        loss_a.append(___)\n",
    "        loss_d.append(___)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the learning process\n",
    "plt.plot(___, label=\"Loss generator\")\n",
    "plt.plot(___,label=\"loss discriminator\")\n",
    "plt.legend()\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('iterations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#generate new time series\n",
    "random_latent_vectors = np.___.___(size=(1, latent_dim))\n",
    "gen_ts = generator.predict(___)\n",
    "plt.plot(pd.DataFrame(gen_ts).T)\n",
    "plt.xlabel(\"hour\")\n",
    "plt.ylabel(\"dayahead energy price\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#1) \n",
    "import pandas as ___\n",
    "import numpy as ___\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras import backend as K\n",
    "from keras.models import ___\n",
    "import numpy as ___\n",
    "import tensorflow as tf \n",
    "import matplotlib.___ as ___\n",
    "\n",
    "#read in the data\n",
    "shapes = pd.read_csv('Dataset_dayahead.csv')\n",
    "\n",
    "#split the data in train and test datset and convert the data to float32\n",
    "x_train = np.array(shapes.iloc[0:365 * ___ + ___, :]).astype(___.___)\n",
    "x_test = np.array(shapes.iloc[365 * ___ - 1:365 * ___]).astype(___.___)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#2)\n",
    "\n",
    "#define the parameters of the vae\n",
    "input_shape = (x_train.shape[1], )\n",
    "latent_dim = 8\n",
    "batch_size = 16\n",
    "epochs = 50\n",
    "\n",
    "#define the encoder\n",
    "inputs = keras.Input(shape=input_shape)\n",
    "x = layers.___(128, activation='___')(inputs)\n",
    "x = layers.___(64, activation='___')(x)\n",
    "x = layers.___(32, activation='___')(x)\n",
    "\n",
    "z_mean = layers.Dense(___)(x)\n",
    "z_log_var = layers.Dense(___)(x)\n",
    "\n",
    "\n",
    "#reparametrization trick\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    batch = tf.shape(z_mean)[0]\n",
    "    dim = tf.shape(z_mean)[1]\n",
    "    epsilon = tf.random.normal(shape=(batch, dim), mean=0., stddev=1.0)\n",
    "    return z_mean + K.exp(z_log_var) * ___\n",
    "\n",
    "# Use Lambda layer to include the sampling function in the model\n",
    "z = layers.Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "\n",
    "# Encoder model\n",
    "encoder = Model(inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "\n",
    "#define the encoder\n",
    "# This is the input where we will feed `z`.\n",
    "decoder_input = layers.Input(shape=(latent_dim,))\n",
    "x = layers.___(32, activation='___')(___)\n",
    "x = layers.___(64, activation='___')(x)\n",
    "x = layers.___(128, activation='___')(x)\n",
    "x = layers.Dense(input_shape[0], activation='linear')(x)\n",
    "\n",
    "\n",
    "# build the decoder in keras:\n",
    "decoder = Model(decoder_input, x, name=\"decoder\")\n",
    "\n",
    "#decode the latent space variable\n",
    "z_decoded = decoder(___)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Custom VAE model\n",
    "class VAE(Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var, z = self.encoder(inputs)\n",
    "        reconstructed = self.decoder(z)\n",
    "        kl_loss = -0.5 * tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=-1)\n",
    "        self.add_loss(tf.reduce_mean(kl_loss) / tf.cast(tf.shape(inputs)[0], tf.float32))\n",
    "        return reconstructed\n",
    "\n",
    "vae = VAE(encoder, decoder)\n",
    "vae.compile(optimizer='rmsprop', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the VAE\n",
    "vae.fit(x_train, x_train, epochs=epochs, batch_size=batch_size, validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "***\n",
    "[1] Keras online documentation: https://keras.io/about/\n",
    "\n",
    "[2] Rashid, T. (2017). Neuronale Netze selbst programmieren: ein verständlicher Einstieg mit Python. O'Reilly.\n",
    "\n",
    "[3] Nguyen, C. N., & Zeigermann, O. (2018). Machine Learning: kurz & gut.\n",
    "\n",
    "[4] Chollet, F. (2018). Deep Learning mit Python und Keras: Das Praxis-Handbuch vom Entwickler der Keras-Bibliothek. MITP-Verlags GmbH & Co. KG. Code available on: https://github.com/fchollet/deep-learning-with-python-notebooks\n",
    "\n",
    "[5]  Jason Brownlee (2019). https://machinelearningmastery.com/impressive-applications-of-generative-adversarial-networks/\n",
    "\n",
    "[6] Kingma, D. P., & Welling, M. (2013). Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114.\n",
    "\n",
    "[7] Rezende, D. J., Mohamed, S., & Wierstra, D. (2014). Stochastic backpropagation and approximate inference in deep generative models. arXiv preprint arXiv:1401.4082.\n",
    "\n",
    "[8] Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A. & Bengio, Y. (2014). Generative adversarial nets. In Advances in neural information processing systems (pp. 2672-2680)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
