{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## <span style=\"color:#4375c7\">DAI</span>\n",
    "***\n",
    "*Course materials are for educational purposes only. Nothing contained herein should be considered investment advice or an opinion regarding the suitability of any security. For more information about this course, please contact us.*\n",
    "***\n",
    "\n",
    "### Session contents:\n",
    "1. **[Deep Learning - Generative models ](#NN)**\n",
    "    - [Variational autoencoder](#VAE)\n",
    "    - [Generative adversarial network](#GAN)\n",
    "\n",
    "    \n",
    "2. **[Hands-on session](#ho)**\n",
    "***\n",
    "\n",
    "##  Deep Learning - Generative models <a name=\"NN\"></a> \n",
    "\n",
    "**Generative models** can be used to learn the underlying distribution of a data set. With the distribution, we can sample from a latent space of data to create entirely new artificial data. There are various applications for artificial data and generative models, such as generating photographs of human faces or animals, cartoon characters, text to image translation, photos to emojis, clothing translation, music, or time series generation [5].\n",
    "\n",
    "The basic concept of generative models is to create a low-dimensional latent space of representations where each point is mapped to a realistic data point such as an image, time series, or text. To map the latent space to a realistic output, we use neural networks. These mapping networks are also called generators or decoders, as we will see later. After developing the latent space, we can randomly sample points from it and map them to our output to get new artificial data [4]. *(The figure below is taken from Chollet [4])* \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/firrm/DAI/main/assets/data_generation.png\" alt=\"Drawing\" style=\"width: 550px;\"/>\n",
    "\n",
    "\n",
    "\n",
    "We will focus on two generative deep learning models, the generative adversarial network (GAN) and the variational autoencoder (VAE). Both use different strategies to generate new data. While VAEs can easily learn structured latent spaces where different directions encode a meaningful axis of variation in the data, GANs can generate more realistic artificial data but use latent spaces that are less structured [4].\n",
    "\n",
    "During this course, we will use **[keras](https://keras.io/)** for the analysis. Keras is one of the most widely used Python libraries for solving machine learning problems with deep neural networks."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "is_executing": true
    }
   },
   "source": [
    "!pip install -q -r https://raw.githubusercontent.com/firrm/DAI/main/requirements.txt #ensure that the required packages are installed\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "import tensorflow as tf\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational autoencoders <a id='VAE'></a>\n",
    "\n",
    "The **Variational Autoencoder** (VAE) was introduced by  Kingma & Welling [6] and Rezende et al. [7]. \n",
    "\n",
    "A VAE is an extended model of an **autoencoder**. An autoencoder encodes or compresses the input to a fixed code in the **latent space** and decodes it back to the same image. *(The figure below is taken from Chollet [4])* \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/firrm/DAI/main/assets/autoencoder.png\" alt=\"Drawing\" style=\"width: 550px;\"/>\n",
    "\n",
    "\n",
    "\n",
    "Unlike an autoencoder, the VAE encodes the input data into parameters of a statistical distribution. It assumes that the data was generated by a statistical process. After encoding the data into the mean and variance of that distribution, the VAE randomly samples a point from the distribution. This random point is then decoded back to the original input. \n",
    "\n",
    "The stochastic random part of this encoding and decoding process forces the latent space to encode meaningful representations. Each point sampled from the latent space is decoded into an original output [4]. *(The figure below is taken from Chollet [4])* \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/firrm/DAI/main/assets/vae.png\" alt=\"Drawing\" style=\"width: 550px;\"/>\n",
    "\n",
    "\n",
    "Basically, a VAE consists of a pair of two connected neural networks. One neural network, acting as an **encoder**, encodes the data into a representation in the latent space, and the other neural network, acting as an **decoder**, decodes the encoded data to obtain the original data. \n",
    "\n",
    "The VAE is typically trained as a single network and optimized by **back-propagation**. The number of nodes in each layer of the encoder decreases as the encoder learns to discard irrelevant information in the data. \n",
    "The number of nodes of each layer of the decoder increases as the decoder learns to reconstruct the input data from the encoding.\n",
    "\n",
    "Let's start by building a VAE following an example from Chollet [4] for image generation of the **MNIST** handwritten digits dataset. We use layers other than dense layers, called **convolutional** layers, which are efficient and powerful layers for **image processing**. If you want to learn more about convolutional layers and more about neural networks for image data, please take the interactive **DataCamp** course \"Image Processing with Keras in Python\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T11:48:23.295470Z",
     "start_time": "2025-05-05T11:48:23.249146Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "img_shape = (28, 28, 1)\n",
    "batch_size = 16\n",
    "latent_dim = 2  # dimension of the latent space\n",
    "\n",
    "# Build the encoder:\n",
    "input_img = keras.Input(shape=img_shape)\n",
    "\n",
    "x = layers.Conv2D(32, 3, padding='same', activation='relu')(input_img)\n",
    "x = layers.Conv2D(64, 3, padding='same', activation='relu', strides=(2, 2))(x)\n",
    "x = layers.Conv2D(64, 3, padding='same', activation='relu')(x)\n",
    "x = layers.Conv2D(64, 3, padding='same', activation='relu')(x)\n",
    "shape_before_flattening = x.shape[1:]  # get the shape of the data after convolution\n",
    "\n",
    "x = layers.Flatten()(x)  # convert images to flatten vector that you could pass to a Dense layer\n",
    "x = layers.Dense(32, activation='relu')(x)\n",
    "\n",
    "# Parameters of the latent space\n",
    "z_mean = layers.Dense(latent_dim)(x)\n",
    "z_log_var = layers.Dense(latent_dim)(x)\n",
    "\n",
    "# Sampling from the latent space also called reparametrization trick\n",
    "def sampling(args):\n",
    "    \"\"\"Reparameterization trick by sampling from an isotropic unit Gaussian.\n",
    "\n",
    "    Arguments:\n",
    "        args (tensor): Mean and log of variance of Q(z|X)\n",
    "\n",
    "    Returns:\n",
    "        z (tensor): Sampled latent vector\n",
    "    \"\"\"\n",
    "    z_mean, z_log_var = args\n",
    "\n",
    "    batch = tf.shape(z_mean)[0]\n",
    "    dim = tf.shape(z_mean)[1]\n",
    "\n",
    "    # By default, random_normal has mean=0 and std=1.0\n",
    "    epsilon = tf.random.normal(shape=(batch, dim))\n",
    "\n",
    "    return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "\n",
    "z = layers.Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "\n",
    "encoder = Model(input_img, [z_mean, z_log_var, z])\n",
    "\n",
    "# Build the decoder:\n",
    "\n",
    "# Input of the decoder (z).\n",
    "decoder_input = layers.Input(shape=(latent_dim,))\n",
    "\n",
    "# Upsample to the correct number of nodes\n",
    "x = layers.Dense(np.prod(shape_before_flattening),\n",
    "                 activation='relu')(decoder_input)\n",
    "\n",
    "# Reshape into an image of the same shape as before flattening the images in the encoder\n",
    "x = layers.Reshape(shape_before_flattening)(x)\n",
    "\n",
    "# Conv2DTranspose layers do the reverse operation to the convolutional layers:\n",
    "x = layers.Conv2DTranspose(64, 3, padding='same', activation='relu')(x)\n",
    "x = layers.Conv2DTranspose(64, 3, padding='same', activation='relu')(x)\n",
    "x = layers.Conv2DTranspose(32, 3, padding='same', activation='relu')(x)\n",
    "x = layers.Conv2DTranspose(1, 3, padding='same', activation='sigmoid')(x)\n",
    "\n",
    "# We receive a feature map of the same size as the input\n",
    "decoder = Model(decoder_input, x)\n",
    "\n",
    "# We apply the decoder model to z of the latent space to receive the decoded z\n",
    "z_decoded = decoder(encoder(input_img)[2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training procedure can be described as follows:\n",
    "\n",
    "Given an input dataset, the encoder encodes the data points into a mean $\\mu$ and a variance $\\sigma$ parameter of a distribution in the latent space. Then, we generate a random data point \n",
    "\n",
    "$$ z= \\mu + \\sigma*\\epsilon $$\n",
    "\n",
    "from the latent distribution with $\\epsilon \\sim \\mathcal{N}(0,I)$, a random standard normal distributed tensor, and decode that point to the original input data. By multiplying with $\\epsilon$, we can verify that each point close to $z$ can be decoded to an output that is similar to the input data.\n",
    "\n",
    "We minimize the mean of two loss functions: A **reconstruction loss** function that forces the decoded samples to be similar to the input data, and a **regularization loss** function that prevents the autoencoder from overfitting and helps to learn the latent space.\n",
    "\n",
    "We use the **binary cross entropy** as the reconstruction loss function, while we use **Kullback–Leibler divergence** as the regularization loss function.\n",
    "To generate a new sample with a VAE, we select a random point of the learned latent distribution and decode it with the decoder. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T11:48:27.620900Z",
     "start_time": "2025-05-05T11:48:27.588773Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[1mModel: \"functional_2\"\u001B[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>,    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │ input_layer_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>,    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │ conv2d_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>,    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │ conv2d_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>,    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │ conv2d_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12544</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │    <span style=\"color: #00af00; text-decoration-color: #00af00\">401,440</span> │ flatten_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">66</span> │ dense_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">66</span> │ dense_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
       "│                     │                   │            │ dense_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mLayer (type)       \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape     \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m   Param #\u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mConnected to     \u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2       │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m28\u001B[0m, \u001B[38;5;34m28\u001B[0m, \u001B[38;5;34m1\u001B[0m) │          \u001B[38;5;34m0\u001B[0m │ -                 │\n",
       "│ (\u001B[38;5;33mInputLayer\u001B[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_4 (\u001B[38;5;33mConv2D\u001B[0m)   │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m28\u001B[0m, \u001B[38;5;34m28\u001B[0m,    │        \u001B[38;5;34m320\u001B[0m │ input_layer_2[\u001B[38;5;34m0\u001B[0m]… │\n",
       "│                     │ \u001B[38;5;34m32\u001B[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_5 (\u001B[38;5;33mConv2D\u001B[0m)   │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m14\u001B[0m, \u001B[38;5;34m14\u001B[0m,    │     \u001B[38;5;34m18,496\u001B[0m │ conv2d_4[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m]    │\n",
       "│                     │ \u001B[38;5;34m64\u001B[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_6 (\u001B[38;5;33mConv2D\u001B[0m)   │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m14\u001B[0m, \u001B[38;5;34m14\u001B[0m,    │     \u001B[38;5;34m36,928\u001B[0m │ conv2d_5[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m]    │\n",
       "│                     │ \u001B[38;5;34m64\u001B[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_7 (\u001B[38;5;33mConv2D\u001B[0m)   │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m14\u001B[0m, \u001B[38;5;34m14\u001B[0m,    │     \u001B[38;5;34m36,928\u001B[0m │ conv2d_6[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m]    │\n",
       "│                     │ \u001B[38;5;34m64\u001B[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten_1 (\u001B[38;5;33mFlatten\u001B[0m) │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m12544\u001B[0m)     │          \u001B[38;5;34m0\u001B[0m │ conv2d_7[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_4 (\u001B[38;5;33mDense\u001B[0m)     │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m32\u001B[0m)        │    \u001B[38;5;34m401,440\u001B[0m │ flatten_1[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_5 (\u001B[38;5;33mDense\u001B[0m)     │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m2\u001B[0m)         │         \u001B[38;5;34m66\u001B[0m │ dense_4[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_6 (\u001B[38;5;33mDense\u001B[0m)     │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m2\u001B[0m)         │         \u001B[38;5;34m66\u001B[0m │ dense_4[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_1 (\u001B[38;5;33mLambda\u001B[0m)   │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m2\u001B[0m)         │          \u001B[38;5;34m0\u001B[0m │ dense_5[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m],    │\n",
       "│                     │                   │            │ dense_6[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m0\u001B[0m]     │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">494,244</span> (1.89 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m494,244\u001B[0m (1.89 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">494,244</span> (1.89 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m494,244\u001B[0m (1.89 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[1mModel: \"functional_3\"\u001B[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12544</span>)          │        <span style=\"color: #00af00; text-decoration-color: #00af00\">37,632</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ reshape_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_transpose_4              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_transpose_5              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_transpose_6              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,464</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_transpose_7              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │           <span style=\"color: #00af00; text-decoration-color: #00af00\">289</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2DTranspose</span>)               │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mLayer (type)                   \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape          \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      Param #\u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_3 (\u001B[38;5;33mInputLayer\u001B[0m)      │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m2\u001B[0m)              │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (\u001B[38;5;33mDense\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m12544\u001B[0m)          │        \u001B[38;5;34m37,632\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ reshape_1 (\u001B[38;5;33mReshape\u001B[0m)             │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m14\u001B[0m, \u001B[38;5;34m14\u001B[0m, \u001B[38;5;34m64\u001B[0m)     │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_transpose_4              │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m14\u001B[0m, \u001B[38;5;34m14\u001B[0m, \u001B[38;5;34m64\u001B[0m)     │        \u001B[38;5;34m36,928\u001B[0m │\n",
       "│ (\u001B[38;5;33mConv2DTranspose\u001B[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_transpose_5              │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m14\u001B[0m, \u001B[38;5;34m14\u001B[0m, \u001B[38;5;34m64\u001B[0m)     │        \u001B[38;5;34m36,928\u001B[0m │\n",
       "│ (\u001B[38;5;33mConv2DTranspose\u001B[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_transpose_6              │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m14\u001B[0m, \u001B[38;5;34m14\u001B[0m, \u001B[38;5;34m32\u001B[0m)     │        \u001B[38;5;34m18,464\u001B[0m │\n",
       "│ (\u001B[38;5;33mConv2DTranspose\u001B[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_transpose_7              │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m14\u001B[0m, \u001B[38;5;34m14\u001B[0m, \u001B[38;5;34m1\u001B[0m)      │           \u001B[38;5;34m289\u001B[0m │\n",
       "│ (\u001B[38;5;33mConv2DTranspose\u001B[0m)               │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">130,241</span> (508.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m130,241\u001B[0m (508.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">130,241</span> (508.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m130,241\u001B[0m (508.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class CustomVariationalLayer(keras.layers.Layer):\n",
    "\n",
    "    def vae_loss(self, x, z_decoded):\n",
    "        x = K.flatten(x)\n",
    "        z_decoded = K.flatten(z_decoded)\n",
    "        xent_loss = keras.metrics.binary_crossentropy(x, z_decoded)\n",
    "        kl_loss = -5e-4 * K.mean(\n",
    "            1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "        return K.mean(xent_loss + kl_loss)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs[0]\n",
    "        z_decoded = inputs[1]\n",
    "        loss = self.vae_loss(x, z_decoded)\n",
    "        self.add_loss(loss, inputs=inputs)\n",
    "        return x\n",
    "\n",
    "# We call our custom layer on the input and the decoded output,\n",
    "# to obtain the final model output.\n",
    "\n",
    "#Instead of defining a loss function we use a trick: a custom last layer that works as loss function\n",
    "#y = CustomVariationalLayer()([input_img, z_decoded])\n",
    "\n",
    "#build the vae model in keras and print the summary\n",
    "#vae = Model(input_img, y)\n",
    "y = CustomVariationalLayer()([input_img, z_decoded])\n",
    "vae = Model(input_img, y)\n",
    "vae.compile(optimizer='rmsprop', loss=None)\n",
    "encoder.summary()\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read in the data and fit the VAE model. Fitting a VAE can be very costly in terms of computing power and time. Please use a GPU processor if available. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T11:49:56.057434Z",
     "start_time": "2025-05-05T11:49:55.881841Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Exception encountered when calling CustomVariationalLayer.call().\n\n\u001B[1mCould not automatically infer the output shape / dtype of 'custom_variational_layer_1' (of type CustomVariationalLayer). Either the `CustomVariationalLayer.call()` method is incorrect, or you need to implement the `CustomVariationalLayer.compute_output_spec() / compute_output_shape()` method. Error encountered:\n\nmodule 'keras.api.backend' has no attribute 'reshape'\u001B[0m\n\nArguments received by CustomVariationalLayer.call():\n  • args=(['<KerasTensor shape=(None, 28, 28, 1), dtype=float32, sparse=False, ragged=False, name=keras_tensor_21>', '<KerasTensor shape=(None, 14, 14, 1), dtype=float32, sparse=False, ragged=False, name=keras_tensor_41>'],)\n  • kwargs=<class 'inspect._empty'>",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mAttributeError\u001B[39m                            Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[7]\u001B[39m\u001B[32m, line 32\u001B[39m\n\u001B[32m     28\u001B[39m         \u001B[38;5;28mself\u001B[39m.add_loss(loss, inputs=inputs)\n\u001B[32m     29\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m x\n\u001B[32m---> \u001B[39m\u001B[32m32\u001B[39m y = \u001B[43mCustomVariationalLayer\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43minput_img\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mz_decoded\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     33\u001B[39m vae = Model(input_img, y)\n\u001B[32m     34\u001B[39m vae.compile(optimizer=\u001B[33m'\u001B[39m\u001B[33mrmsprop\u001B[39m\u001B[33m'\u001B[39m, loss=\u001B[38;5;28;01mNone\u001B[39;00m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.cache/pypoetry/virtualenvs/dai_lecture_repo-AuS8k46r-py3.12/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001B[39m, in \u001B[36mfilter_traceback.<locals>.error_handler\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    119\u001B[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001B[32m    120\u001B[39m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[32m    121\u001B[39m     \u001B[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m122\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m e.with_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    123\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    124\u001B[39m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[7]\u001B[39m\u001B[32m, line 27\u001B[39m, in \u001B[36mCustomVariationalLayer.call\u001B[39m\u001B[34m(self, inputs)\u001B[39m\n\u001B[32m     25\u001B[39m x = inputs[\u001B[32m0\u001B[39m]\n\u001B[32m     26\u001B[39m z_decoded = inputs[\u001B[32m1\u001B[39m]\n\u001B[32m---> \u001B[39m\u001B[32m27\u001B[39m loss = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mvae_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mz_decoded\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     28\u001B[39m \u001B[38;5;28mself\u001B[39m.add_loss(loss, inputs=inputs)\n\u001B[32m     29\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[7]\u001B[39m\u001B[32m, line 17\u001B[39m, in \u001B[36mCustomVariationalLayer.vae_loss\u001B[39m\u001B[34m(self, x, z_decoded)\u001B[39m\n\u001B[32m     16\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mvae_loss\u001B[39m(\u001B[38;5;28mself\u001B[39m, x, z_decoded):\n\u001B[32m---> \u001B[39m\u001B[32m17\u001B[39m     x = \u001B[43mK\u001B[49m\u001B[43m.\u001B[49m\u001B[43mreshape\u001B[49m(x, shape=(-\u001B[32m1\u001B[39m,))\n\u001B[32m     18\u001B[39m     z_decoded = K.reshape(z_decoded, shape=(-\u001B[32m1\u001B[39m,))\n\u001B[32m     19\u001B[39m     xent_loss = keras.metrics.binary_crossentropy(x, z_decoded)\n",
      "\u001B[31mAttributeError\u001B[39m: Exception encountered when calling CustomVariationalLayer.call().\n\n\u001B[1mCould not automatically infer the output shape / dtype of 'custom_variational_layer_1' (of type CustomVariationalLayer). Either the `CustomVariationalLayer.call()` method is incorrect, or you need to implement the `CustomVariationalLayer.compute_output_spec() / compute_output_shape()` method. Error encountered:\n\nmodule 'keras.api.backend' has no attribute 'reshape'\u001B[0m\n\nArguments received by CustomVariationalLayer.call():\n  • args=(['<KerasTensor shape=(None, 28, 28, 1), dtype=float32, sparse=False, ragged=False, name=keras_tensor_21>', '<KerasTensor shape=(None, 14, 14, 1), dtype=float32, sparse=False, ragged=False, name=keras_tensor_41>'],)\n  • kwargs=<class 'inspect._empty'>"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "# Train the vae model on MNIST handwritten digits\n",
    "(x_train, _), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_train = x_train.reshape(x_train.shape + (1,))\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_test = x_test.reshape(x_test.shape + (1,))\n",
    "\n",
    "# Update VAE model with custom loss and compile\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "class CustomVariationalLayer(keras.layers.Layer):\n",
    "    def vae_loss(self, x, z_decoded):\n",
    "        x = K.reshape(x, shape=(-1,))\n",
    "        z_decoded = K.reshape(z_decoded, shape=(-1,))\n",
    "        xent_loss = keras.metrics.binary_crossentropy(x, z_decoded)\n",
    "        kl_loss = -5e-4 * K.mean(\n",
    "            1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "        return K.mean(xent_loss + kl_loss)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs[0]\n",
    "        z_decoded = inputs[1]\n",
    "        loss = self.vae_loss(x, z_decoded)\n",
    "        self.add_loss(loss, inputs=inputs)\n",
    "        return x\n",
    "\n",
    "\n",
    "y = CustomVariationalLayer()([input_img, z_decoded])\n",
    "vae = Model(input_img, y)\n",
    "vae.compile(optimizer='rmsprop', loss=None)\n",
    "\n",
    "vae.fit(x=x_train, y=None,\n",
    "        shuffle=True,\n",
    "        epochs=10,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(x_test, None))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After fitting our model, we want to generate images from the structured latent space and plot them in a grid of 10 decoded images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T11:50:05.353268Z",
     "start_time": "2025-05-05T11:50:05.245459Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 61ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 196 into shape (28,28)",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[8]\u001B[39m\u001B[32m, line 21\u001B[39m\n\u001B[32m     19\u001B[39m         z_sample = np.tile(z_sample, batch_size).reshape(batch_size, \u001B[32m2\u001B[39m)\n\u001B[32m     20\u001B[39m         x_decoded = decoder.predict(z_sample, batch_size=batch_size)\n\u001B[32m---> \u001B[39m\u001B[32m21\u001B[39m         digit = \u001B[43mx_decoded\u001B[49m\u001B[43m[\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m.\u001B[49m\u001B[43mreshape\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdigit_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdigit_size\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     22\u001B[39m         figure[i * digit_size: (i + \u001B[32m1\u001B[39m) * digit_size,\n\u001B[32m     23\u001B[39m                j * digit_size: (j + \u001B[32m1\u001B[39m) * digit_size] = digit\n\u001B[32m     25\u001B[39m plt.figure(figsize=(\u001B[32m10\u001B[39m, \u001B[32m10\u001B[39m))\n",
      "\u001B[31mValueError\u001B[39m: cannot reshape array of size 196 into shape (28,28)"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "\n",
    "n = 10  # number of digits we want to plot\n",
    "digit_size = 28\n",
    "figure = np.zeros((digit_size * n, digit_size * n))\n",
    "# Linearly spaced coordinates on the unit square were transformed\n",
    "# through the inverse CDF (ppf) of the Gaussian\n",
    "# to produce values of the latent variables z,\n",
    "# since the prior of the latent space is Gaussian [4]\n",
    "grid_x = norm.ppf(np.linspace(0.05, 0.95, n))\n",
    "grid_y = norm.ppf(np.linspace(0.05, 0.95, n))\n",
    "\n",
    "#plot a grid from decoded images from the latent space\n",
    "for i, yi in enumerate(grid_x):\n",
    "    for j, xi in enumerate(grid_y):\n",
    "        z_sample = np.array([[xi, yi]])\n",
    "        z_sample = np.tile(z_sample, batch_size).reshape(batch_size, 2)\n",
    "        x_decoded = decoder.predict(z_sample, batch_size=batch_size)\n",
    "        digit = x_decoded[0].reshape(digit_size, digit_size)\n",
    "        figure[i * digit_size: (i + 1) * digit_size,\n",
    "               j * digit_size: (j + 1) * digit_size] = digit\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(figure, cmap='Greys_r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We receive a grid of the sampled images, which provides us with a continuous distribution of the digit classes of the data set. You can even follow the paths through the latent space, following the grid axis of morphing digits [4]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generative adversarial  networks <a id='GAN'></a>\n",
    "\n",
    "A **generative adversarial network** (GAN) is a generative model introduced by Goodfellow et al. [8], where two neural networks are trained simultaneously.\n",
    "The first network is called the **generator** G, which takes a random point of the **latent space** as an input and captures the true distribution of the data. The second network is the **discriminator** D, which estimates the probability that a sample came from the real data instead of the synthetic data produced by G. \n",
    "\n",
    "The training process can be stated as follows:\n",
    "\n",
    "G and D play a **two-player minimax game** against each other to optimize the model. Therefore, the training procedure of G is basically maximizing the probability that D is making a failure. G learns to capture any implicit distribution from a training data set and provides a way to draw samples from that distribution. Simultaneously, we train D to maximize the probability of assigning the correct label to training examples and samples from G.\n",
    "\n",
    "Let $x \\sim \\mathcal{D}$ be our given data set. We assume some prior distribution $z \\sim p_z(z)$ that G should learn such that $G(z)\\sim p_{data}(x)$. Let $V(G,D)$ be our value function. We then want G and D to play the following minimax game:\n",
    "\n",
    "\\begin{equation}\n",
    "\\min_G \\max_D V(G,D) = \\mathbb{E}_{x \\sim p_{data}(x)} [log D(x)] + \\mathbb{E}_{z \\sim p_{z}(z)} [log(1-D(G(z))]\n",
    "\\end{equation}\n",
    "\n",
    "To simulate new samples from G, we calculate random normal data points $z$ from the latent space and use the trained generator function $G(z)$ to generate a new sample $x$  [8], [4]. \n",
    "\n",
    "Let's start to construct the GAN inspired by Chollet [4] for image generation and use it for the *kaggle* cat images data set. So, we basically use it to generate pictures of cats.\n",
    "\n",
    "We start with the **generator**. The generator model transforms a random vector from the latent space to an artificial data point. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "latent_dim = 64\n",
    "height = 64\n",
    "width = 64\n",
    "channels = 1\n",
    "\n",
    "generator_input = keras.Input(shape=(latent_dim,))\n",
    "\n",
    "# First, transform the input into a 16x16 128-channels feature map\n",
    "x = layers.Dense(128 * 16 * 16)(generator_input)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Reshape((16, 16, 128))(x)\n",
    "\n",
    "# Then, add a convolution layer\n",
    "x = layers.Conv2D(256, 5, padding='same')(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "\n",
    "# Upsample to 32x32\n",
    "x = layers.Conv2DTranspose(256, 5, strides=2, padding='same')(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "\n",
    "# Upsample to 32x32\n",
    "x = layers.Conv2DTranspose(256, 5, strides=2, padding='same')(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "\n",
    "# Few more conv layers\n",
    "x = layers.Conv2D(256, 5, padding='same')(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "\n",
    "# Produce a 32x32 1-channel feature map\n",
    "x = layers.Conv2D(channels, 7, activation='tanh', padding='same')(x)\n",
    "generator = keras.models.Model(generator_input, x)\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The **discriminator** takes the candidate data point (real or fake)  as an input and classifies it. So it decides whether it is a generated data point or a point from the training set. \n",
    "\n",
    "We use the **sigmoid activation** function in the last layer as the discriminator performs a **binary classification** task [4]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator_input = layers.Input(shape=(height, width, channels))\n",
    "x = layers.Conv2D(128, 3)(discriminator_input)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Conv2D(128, 4, strides=2)(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Conv2D(128, 4, strides=2)(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Conv2D(128, 4, strides=2)(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Flatten()(x)\n",
    "\n",
    "# One dropout layer - important trick!\n",
    "x = layers.Dropout(0.4)(x)\n",
    "\n",
    "# Classification layer\n",
    "x = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "discriminator = keras.models.Model(discriminator_input, x)\n",
    "discriminator.summary()\n",
    "\n",
    "# To stabilize training, we use learning rate decay\n",
    "# and gradient clipping (by value) in the optimizer.\n",
    "discriminator_optimizer = keras.optimizers.RMSprop(learning_rate=0.0008, clipvalue=1.0)\n",
    "discriminator.compile(optimizer=discriminator_optimizer, loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final GAN model chains the generator and the discriminator together: GAN(x) = D(G(x)). The GAN maps the latent space vectors to the classification of the discriminator. The discriminator should be frozen during the training process of the generator as the GAN updates the weights of the generator in a way that makes the discriminator predict more likely \"real\" when classifying artificial data points [4]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator.trainable = False\n",
    "\n",
    "gan_input = keras.Input(shape=(latent_dim,))\n",
    "gan_output = discriminator(generator(gan_input))\n",
    "gan = keras.models.Model(gan_input, gan_output)\n",
    "\n",
    "gan_optimizer = keras.optimizers.RMSprop(learning_rate=0.0004, clipvalue=1.0, decay=1e-8)\n",
    "gan.compile(optimizer=gan_optimizer, loss='binary_crossentropy')\n",
    "gan.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each epoch in the training process consists of the following steps:\n",
    "    \n",
    "    1. Draw random points from the latent space using the standard normal distribution\n",
    "    2. Generate artificial data points with the generator\n",
    "    3. Mix the generated data points with the real data points from the training set\n",
    "    4. Unfreeze the discriminator weights\n",
    "    5. Train the discriminator with the mixed data points\n",
    "    6. Draw new random points from the latent space\n",
    "    7. Freeze the discriminator weights\n",
    "    8. Train the whole GAN with the random vectors and targets that always return true. This will update the weights of the generator in a way that the discriminator will predict generated data points as real data points.\n",
    "\n",
    "\n",
    "The training of a GAN is difficult because it is a dynamic process and not just a simple gradient descent process with a fixed loss landscape. The correct training of a GAN requires an extensive tuning and adjustment of the GAN architecture [4]. *(The figure below is taken from Chollet [4])* \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/firrm/DAI/main/assets/GAN.png\">\n",
    "\n",
    "\n",
    "Let's train the GAN model on the **kaggle** cats data set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing import image\n",
    "import pickle\n",
    "\n",
    "with open(\"../data/cat_dataset_64x64.pickle\", \"rb\") as file:\n",
    "    x_train = pickle.load(file)\n",
    "\n",
    "x_train = x_train.reshape(-1,64,64,1)       \n",
    "x_train = x_train / 127.5 - 1.  # values -1 to 1\n",
    "\n",
    "iterations = 1000\n",
    "batch_size = 20\n",
    "\n",
    "\n",
    "\n",
    "# Start training loop\n",
    "start = 0\n",
    "for step in range(iterations):\n",
    "    # Sample random points in the latent space\n",
    "    random_latent_vectors = np.random.normal(size=(batch_size, latent_dim))\n",
    "    \n",
    "    # Decode them to fake images\n",
    "    generated_images = generator.predict(random_latent_vectors)\n",
    "\n",
    "    # Combine them with real images\n",
    "    stop = start + batch_size\n",
    "    real_images = x_train[start: stop]\n",
    "\n",
    "    combined_images = np.concatenate([generated_images, real_images])\n",
    "\n",
    "    # Assemble labels discriminating real from fake images\n",
    "    labels = np.concatenate([np.ones((batch_size, 1)),\n",
    "                             np.zeros((batch_size, 1))])\n",
    "    # Add random noise to the labels - important trick!\n",
    "    labels += 0.05 * np.random.random(labels.shape)\n",
    "    \n",
    "    discriminator.trainable = True\n",
    "    # Train the discriminator\n",
    "    d_loss = discriminator.train_on_batch(combined_images, labels)\n",
    "\n",
    "    # sample random points in the latent space\n",
    "    random_latent_vectors = np.random.normal(size=(batch_size, latent_dim))\n",
    "    discriminator.trainable = False\n",
    "    # Assemble labels that say \"all real images\"\n",
    "    misleading_targets = np.zeros((batch_size, 1))\n",
    "\n",
    "    # Train the generator (via the gan model,\n",
    "    # where the discriminator weights are frozen)\n",
    "    a_loss = gan.train_on_batch(random_latent_vectors, misleading_targets)\n",
    "    \n",
    "    start += batch_size\n",
    "    if start > len(x_train) - batch_size:\n",
    "      start = 0\n",
    "\n",
    "    # Occasionally save / plot\n",
    "    if step % 100 == 0:\n",
    "        # Save model weights\n",
    "        gan.save_weights('gan.h5')\n",
    "\n",
    "        # Print metrics\n",
    "        print('discriminator loss at step %s: %s' % (step, d_loss))\n",
    "        print('adversarial loss at step %s: %s' % (step, a_loss))\n",
    "\n",
    "        # Save one generated image\n",
    "        img = image.array_to_img(generated_images[0] * 255., scale=False)\n",
    "        #img.save(os.path.join(save_dir, 'generated_frog' + str(step) + '.png'))\n",
    "        plt.figure()\n",
    "        plt.imshow(img,cmap='gray')\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "plt.plot(a_loss, label=\"Loss generator\")\n",
    "plt.plot(d_loss, label=\"loss discriminator\")\n",
    "plt.legend()\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('iterations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We sum up some tips following Chollet [4] for the training process of a GAN:\n",
    "\n",
    "- Sample points from the latent space using a normal distribution\n",
    "\n",
    "- To induce robustness use stochasticity. GANs might get stuck during the training as it results in a dynamic equilibrium. Therefore introduce randomness by using a dropout layer in the discriminator and adding random noise to the labels for the discriminator.\n",
    "\n",
    "- Use Leaky ReLu activation functions to prevent sparse gradients during the GAN training\n",
    "\n",
    "- Use a lower learning rate for the generator as for the discriminator\n",
    "\n",
    "Let's plot some of the artificial cat images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample random points in the latent space\n",
    "random_latent_vectors = np.random.normal(size=(10, latent_dim))\n",
    "\n",
    "# Decode them to fake images\n",
    "generated_images = generator.predict(random_latent_vectors)\n",
    "\n",
    "for i in range(generated_images.shape[0]):\n",
    "    img = image.array_to_img(generated_images[i] * 255., scale=True)\n",
    "    plt.figure()\n",
    "    plt.imshow(img,cmap='gray')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "\n",
    "### Session takeaways \n",
    "\n",
    "*What are the learnings of this session?*\n",
    "\n",
    "- you know the structure and training of a variational autoencoder and  a generative adversarial network\n",
    "- you know how to generate new data with a variational autoencoder and a generative adversarial network\n",
    "\n",
    "\n",
    "*What's next?*\n",
    "\n",
    "During the hands-on session, you are using Keras to build your own VAE and GAN for time series generation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
