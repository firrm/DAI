{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNlup4UWEQUvG49wUXNJtb2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Dieses Notebook bildet den Abschluss der Datenverarbeitung. Hier werden die Daten in Zeitreihen geschrieben, sodass wir als Spalten unsere einzelnen Features haben und dann eine Zeile ein Trainingsdatensatz ist. Damit die folgenden Code Zeilen leichter verständlich sind, ist hier eine Übersicht mit den Spalten, die wir hinterher haben werden und deren Bedeutung / Berechnung:"],"metadata":{"id":"Venf5tCRWDzd"}},{"cell_type":"markdown","source":["•\tda_prc.YEAR, das ist das jahr\n","\n","•\tda_prc.WEEKDAY, das ist der tag (1, 2, .., 31)\n","\n","•\tda_prc.MONTH, das ist der Monat\n","\n","•\tda_prc.HOUR, das ist die Stunde\n","\n","•\tda_prc.DUMMY_WEEKDAY, (1 für Werktag, 2 für Wochenende)\n","\n","•\tda_prc.DA_PRC_DAY_BEFORE, Day Ahead Preis vom Vortag für die Stunde\n","\n","•\tda_prc.DA_AV_PRC_3_DAYS, durchschnittlicher Day Ahead Preis der letzten 3 tage, (für montag 11 uhr wäre das durchschnitt freitag 11 uhr, donnerstag 11 uhr, mittwoch 11 uhr)\n","\n","•\tda_prc.DA_PRC_SAME_DAY, day ahead preis für den tag, der forgeacastet werden soll\n","\n","•\tda_vol.DA_VOL_DAY_BEFORE, analog zu DA_PRC\n","\n","•\tda_vol.DA_VOL_SAME_DAY, analog zu DA_PRC\n","\n","•\tda_vol.DA_AV_VOL_3_DAYS, analog zu DA_PRC\n","\n","•\tpl_pr.PL_PR_TOTAL, geplante totale Produktion für die Stunde\n","\n","•\tpl_pr.PL_PR_GAS, geplante MWh produktion durch Gas für die Stunde\n","\n","•\tpl_pr.PL_PR_WIND, etc.\n","\n","•\tpl_pr.PL_PR_LIGNITE,etc.\n","\n","•\tpl_pr.PL_PR_BC, etc.\n","\n","•\tpl_pr.PL_PR_IC, etc.\n","\n","•\tpl_pr.PL_PR_OIL, etc.\n","\n","•\tpl_pr.PL_PR_GEO, etc.\n","\n","•\tpl_pr.PL_PR_HYDRO, etc.\n","\n","•\tpl_pr.PL_PR_NAPHTA, etc.\n","\n","•\tpl_pr.PL_PR_BIO, etc.\n","\n","•\tpl_pr.PL_PR_RIVER, etc.\n","\n","•\tpl_pr.PL_PR_OTHER,etc.\n","\n","•\tpr_inj.INJ_SUN_{H}_h, Solar MWh, H kann hier 1, 2 oder 3 sein. Wenn H = 3 ist, dann ist in diesem Feature die Solar einspeisung von vor 3 Stunden. D.h wenn wir Stunde 11 vorhersagen wollen, dann ist das die Solareinspeisung von 7-8\n","\n","•\tpr_inj.INJ_ASPHAL_{H}_h, etc.\n","\n","•\tpr_inj.INJ_LNG_{H}_h, etc.\n","\n","•\tpr_inj.INJ_II_{H}_h, etc. (International Import)\n","\n","•\tpr_inj.INJ_IE_{H}_h, etc. (International Export)\n","\n","•\tpr_inj.DIFFERENZ_PR_INJ_TOTAL_{H}_h, Differenz der totalen Produktion von vor H stunden für die Stunde die wir vorhersagen wollen\n","\n","•\tpr_inj.DIFFERENZ_PR_INJ_GAS_{H}_h, etc.\n","\n","•\tpr_inj.DIFFERENZ_PR_INJ_WIND_{H}_h, etc.\n","\n","•\tpr_inj.DIFFERENZ_PR_INJ_LIGNITE_{H}_h, etc.\n","\n","•\tpr_inj.DIFFERENZ_PR_INJ_BC_{H}_h, etc.\n","\n","•\tpr_inj.DIFFERENZ_PR_INJ_IC_{H}_h, etc.\n","\n","•\tpr_inj.DIFFERENZ_PR_INJ_OIL_{H}_h, etc.\n","\n","•\tpr_inj.DIFFERENZ_PR_INJ_GEO_{H}_h, etc.\n","\n","•\tpr_inj.DIFFERENZ_PR_INJ_HYDRO_{H}_h, etc.\n","\n","•\tpr_inj.DIFFERENZ_PR_INJ_NAPHTA_{H}_h, etc.\n","\n","•\tpr_inj.DIFFERENZ_PR_INJ_BIO_{H}_h, etc.\n","\n","•\tpr_inj.DIFFERENZ_PR_INJ_RIVER_{H}_h, etc.\n","\n","•\tpr_inj.DIFFERENZ_PR_INJ_OTHER_{H}_h,etc.\n","\n","•\tpr_inj_rw.ROLLING_WINDOW_INJ_SUN_{H}_h, Durchschnittloche Produktion Solar der letzten 3 Stunden von vor H Stunden vor Schätzung, d.h. für Stunde 11 ist das die durchschnittliche Solar Produktion von 5-8\n","\n","•\tpr_inj_rw.ROLLING_WINDOW_INJ_ASPHAL_{H}_h, etc.\n","\n","•\tpr_inj_rw.ROLLING_WINDOW_INJ_LNG_{H}_h, etc.\n","\n","•\tpr_inj_rw.ROLLING_WINDOW_INJ_II_{H}_h, etc.\n","\n","•\tpr_inj_rw.ROLLING_WINDOW_INJ_IE_{H}_h,etc.\n","\n","•\tpr_inj_rw.ROLLING_WINDOW_DIFFERENZ_PR_INJ_TOTAL_{H}_h, etc.\n","\n","•\tpr_inj_rw.ROLLING_WINDOW_DIFFERENZ_PR_INJ_GAS_{H}_h, etc.\n","\n","•\tpr_inj_rw.ROLLING_WINDOW_DIFFERENZ_PR_INJ_WIND_{H}_h,etc.\n","\n","•\tpr_inj_rw.ROLLING_WINDOW_DIFFERENZ_PR_INJ_LIGNITE_{H}_h,etc.\n","\n","•\tpr_inj_rw.ROLLING_WINDOW_DIFFERENZ_PR_INJ_BC_{H}_h, etc.\n","\n","•\tpr_inj_rw.ROLLING_WINDOW_DIFFERENZ_PR_INJ_IC_{H}_h, etc.\n","\n","•\tpr_inj_rw.ROLLING_WINDOW_DIFFERENZ_PR_INJ_OIL_{H}_h, etc.\n","\n","•\tpr_inj_rw.ROLLING_WINDOW_DIFFERENZ_PR_INJ_GEO_{H}_h, etc.\n","\n","•\tpr_inj_rw.ROLLING_WINDOW_DIFFERENZ_PR_INJ_HYDRO_{H}_h,etc.\n","\n","•\tpr_inj_rw.ROLLING_WINDOW_DIFFERENZ_PR_INJ_NAPHTA_{H}_h,etc.\n","\n","•\tpr_inj_rw.ROLLING_WINDOW_DIFFERENZ_PR_INJ_BIO_{H}_h, etc.\n","\n","•\tpr_inj_rw.ROLLING_WINDOW_DIFFERENZ_PR_INJ_RIVER_{H}_h, etc.\n","\n","•\tpr_inj_rw.ROLLING_WINDOW_DIFFERENZ_PR_INJ_OTHER_{H}_h,etc.\n","\n","•\tfor_load.FORECAST_LOAD_SAME_DAY, vorhergesagte Nachfrage für den Tag\n","\n","•\tfor_load_rw.ROLLING_WINDOW_PAST_{window_size_rolling_window}_h_DIFFERENZ_FOR_REAL_LOAD_{H}_h, etc.\n","\n","•\tsys_dir.SYSTEM_DIRECTION_{H}_h, entweder ist zu wenig energie am markt (-1), genug = 0, oder zu viel = 1\n","\n","•\tsys_dir.ROLLING_WINDOW_PAST_{window_size_rolling_window}_h_SYSTEM_DIRECTION_{H}_h, durchschn. system direction der letzten 3 stunden H stunden vor schätzung\n","\n","•\tup_reg.UP_REG_{H}_h_0, energie die eingeschaltet wurde in stufe 0, damit mehr energie am markt ist\n","\n","up_reg.UP_REG_{H}_h_1, stufe 1\n","\n","•\tup_reg.UP_REG_{H}_h_2, stufe 2\n","\n","•\tdown_reg.DOWN_REG_{H}_h_0, damit weniger energie am markt\n","\n","down_reg.DOWN_REG_{H}_h_1,\n","\n","•\tdown_reg.DOWN_REG_{H}_h_2,\n","\n","•\tfi_pl.PL_FI_LOAD, geplanter nachfrage ausfall\n","\n","•\tfi_unpl.UNPL_FI_LOAD_{H}_h, ungeplanter nachfrage ausfall\n","\n","•\tfi_unpl.ROLLING_WINDOW_PAST_{window_size_rolling_window}_h_UNPL_FI_LOAD_{H}_h,\n","\n","•\tfi_pow.POWER_FI_{H}_h, ungeplanter produzenten ausfall\n","\n","•\tfi_pow.ROLLING_WINDOW_PAST_{window_size_rolling_window}_h_POWER_FI_{H}_h,\n","\n","•\twpp.DIFF_FOR_GEN_WPP_{H}_h, differenz von windproduktion\n","\n","•\twpp.GENERATION_WPP_{H}_h,\n","\n","•\twpp.FORECAST_WPP,\n","\n","•\ttr.TRADE_COUNT_{H}_h, anzahl der trades die für die stunde die wir betrachten gemacht wurden bis vor H stunden vor vorhersage\n","\n","•\ttr.TRADE_VOLATILITY_{H}_h,  s. Count\n","\n","•\ttr.TRADE_VOLUME_{H}_h,  s. Count\n","\n","•\ttr.PRE_ID3_{H}_h, hier wird der ID3 preis H Stunden vor vorhersage geschätzt\n","\n","•\ttr.AV_TRADE_COUNT_{H}_h, Anzahl der Trades H Stunden vor Vorhersage\n","\n","•\ttr.AV_TRADE_VOLATILITY_{H}_h, Durchschnittszahlen der letzten 3 Tage\n","\n","•\ttr.AV_TRADE_VOLUME_{H}_h,\n","\n","•\ttr.AV_PRE_ID3_{H}_h,\n","\n","•\ttr.AV_TRADE_COUNT_{H}_h,\n","\n","•\ttr.AV_TRADE_VOLATILITY_{H}_h,\n","\n","•\ttr.AV_TRADE_VOLUME_{H}_h,\n","\n","•\ttr.AV_PRE_ID3_{H}_h,\n","\n","\n"],"metadata":{"id":"7UzCxKw4aB3j"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yIDGQPH3OFzc","executionInfo":{"status":"ok","timestamp":1715785231232,"user_tz":-120,"elapsed":3727,"user":{"displayName":"Alex Krause","userId":"03640757157319878917"}},"outputId":"0c55e4f6-a563-4c65-916d-89bdfd29587d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","import sys\n","sys.path.append('/content/drive/MyDrive/01_Data_AI_SS24/Project_Turkish_Energy_Market/self_created_python_packages')\n","from tqdm import tqdm\n","from database_functions import get_DataFrame_from_SQL_database, connect_to_database, write_dataframe_to_database\n","from datetime import timedelta\n","import pandas as pd\n","from datetime import datetime\n","# 'contractName' in ein Datum konvertieren\n","import pytz\n","import numpy as np"]},{"cell_type":"code","source":["# connect to database\n","[conn_nonull_, cur_nonull_] = connect_to_database('data_no_null_values.db')\n","\n","startDate_ = \"2021-01-01T00:00:00+03:00\"\n","endDate_ = \"2023-12-31T00:00:00+03:00\"\n","\n","startDate_formatted_ = startDate_[:10].replace(\"-\",\"_\")\n","endDate_formatted_ = endDate_[:10].replace(\"-\",\"_\")\n","\n","# bei manchen Features werden Durchschnitte über die letzten 3 Tage gebildet\n","days_to_take_average_over_ = 3\n","window_size_rolling_window = 3"],"metadata":{"id":"G4t-9PUHliVx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# connect to database where we will store the final dataframes (and then join them together via left join)\n","[conn_timeseries, cur_timeseries] = connect_to_database('timeseries_database.db')"],"metadata":{"id":"9V1kjUg4ryug"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","def get_weekday(df, date_column):\n","  \"\"\"\n","  Funktion um die Wochentage zu bestimmen\n","  \"\"\"\n","  df['python_date'] = pd.to_datetime(df[date_column])\n","  df['WEEKDAY'] = df['python_date'].dt.dayofweek\n","  df['HOUR'] = df['python_date'].dt.hour\n","  df['MONTH'] = df['python_date'].dt.month\n","  df['day'] = df['python_date'].dt.day\n","\n","  df.drop('python_date', axis=1, inplace=True)\n","  return df\n","\n","from numba import njit\n","@njit\n","def get_previous_value_based_on_weekdays(weekday_col, col):\n","  \"\"\"\n","  In dieser Funktion wird basierend auf Wochentagen der vorherige Wert ermittelt, d.h.\n","  für Di, Mi, Do, Fr wird einfach 24h nach hinten gesprungen und der Wert dann eingetragen, für Mo 72h und für Sonn / Samstag 7*24\n","\n","  neuer Wert dann z.B. für\n","\n","  Fr 10.01.2022 11 Uhr   Wert von Donnerstag 09.01.2022 11 uhr\n","\n","  \"\"\"\n","  result = []\n","\n","  for i in range(len(weekday_col)):\n","      if weekday_col[i] == 5 or weekday_col[i] == 6:\n","          if i >= 7*24:\n","              result.append(col[i - 7*24])\n","          else:\n","              result.append(None)\n","      elif weekday_col[i] == 0:\n","          if i >= 3*24:\n","              result.append(col[i - 3*24])\n","          else:\n","              result.append(None)\n","      else:\n","          if i >= 24:\n","              result.append(col[i - 24])\n","          else:\n","              result.append(None)\n","  return result\n","\n","def drop_col_and_add_in_df(df_orig, del_columns = 0, add_columns = 0):\n","  \"\"\"\n","  Hilfsfunktion welche Spalten drin behalten werden sollen und welche dazu kommen\n","  \"\"\"\n","  df = df_orig.copy()\n","  if del_columns != 0:\n","    for col in df.columns:\n","      if col in del_columns:\n","          pass\n","      else:\n","        df.drop(col, axis=1, inplace=True)\n","  if add_columns != 0:\n","    for key, value in add_columns.items():\n","          df[key] = value\n","\n","  return df"],"metadata":{"id":"T3Hb5G4Lru5s"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Average data over past x days"],"metadata":{"id":"sD-XKEHd7s_t"}},{"cell_type":"code","source":["@njit\n","def get_average_over_x_days_based_on_weekdays(weekday_col, col, x_days):\n","  \"\"\"\n","  Diese Funktion berechnet den Durchschnitt der letzten x Tage, basierend auf Wochentagen.\n","  D.h. wenn X = 3 ist und das Datum Freitag 10.01.2022 11 Uhr, soll die Funktion den Mittelwert aus\n","  Di 07.01.2022 11 Uhr, Mi 08.01.2022 11 Uhr und Donnerstag 09.01.2022 11 Uhr berechnen\n","\n","  Wenn X = 3 und wir betrachten den Montag 06.01.2022 11 Uhr, dann nimmt die Funktion den Durchschnitt über\n","  die letzten 3 Werktage, d.h. Freitag 03.01.2022, Do 02.01.2022 und Mi 01.01.2022\n","\n","  Für Wochenenden gilt das selbe, nur dass dort die letzten 3 Wochenendtage genommen werden\n","  \"\"\"\n","  result = []\n","  for i in range(len(weekday_col)):\n","    if weekday_col[i] in [0, 1, 2, 3, 4]:\n","      temp_arr = []\n","      temp_sum = 0\n","      if i >= 24:\n","        k = 1\n","        j = 1\n","        while k < x_days + 1 and i >= j*24:\n","          if i >= j * 24 and weekday_col[i - j * 24] in [0, 1, 2, 3, 4]:\n","            temp_arr.append(j)\n","            k += 1\n","\n","          j += 1\n","\n","        for l in temp_arr:\n","          temp_sum += col[i - l*24]\n","\n","        result.append(temp_sum / (k-1))\n","      else:\n","        result.append(None)\n","\n","    if weekday_col[i] == 5 or weekday_col[i] == 6:\n","      temp_arr = []\n","      temp_sum = 0\n","      if i >= 7*24:\n","        k = 1\n","        j = 1\n","        while k < x_days + 1 and i >= j*24:\n","\n","          if i >= j * 24 and weekday_col[i - j * 24] in [5,6]:\n","            temp_arr.append(j)\n","            k += 1\n","          j += 1\n","\n","        for l in temp_arr:\n","          temp_sum += col[i - l*24]\n","\n","        result.append(temp_sum / (k-1))\n","      else:\n","        result.append(None)\n","\n","  return result\n","\n","def get_difference(col1, col2):\n","  \"\"\"\n","  Funktion die die Differenz zweier Spalten zurückgibt\n","  \"\"\"\n","  return col1 - col2\n","\n","def get_shifted_df_different_shifts(df, col, shift):\n","  \"\"\"\n","  Funktion die eine Spalte in einem DataFrame shiftet, d.h. wenn ich shift = 1 eingebe, wandert jeder Wert um 1 \"nach unten\", d.h. für\n","  Freitag 10.01.2022 11 Uhr steht dann der Wert von Freitag 10.01.2022 10 Uhr\n","  \"\"\"\n","  return df[col].shift(shift).values\n","\n","def shift_df_except_col(df_copy, shift, except_col = ['date']):\n","  df = df_copy\n","  for col in df.columns:\n","    if col in except_col:\n","      pass\n","    else:\n","      df[col] = df[col].shift(shift)\n","\n","  return df"],"metadata":{"id":"lOFPNZ95tnD_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Day Ahead Prices:\n","\n","Achtung: wir nehmen hier den euro wert, rechne den ID3 preis in euro um\n","\n","\n","We want to have:\n","\n","\n","*   DA_PRC_SAME_DAY (Day ahead price from same day)\n","*   DA_PRC_DAY_BEFORE (Day ahead price day before, hier wird aaber zusätzlich noch aufs wochenende geschaut, d.h. ein Samstag bekommt den letzten preis eines samstages, nicht die vom freitag)\n","*   DA_AV_PRC_X_DAYS (Durchschnitt des day ahead prc der letzten X tage)\n","\n"],"metadata":{"id":"4ShOYqHtFm29"}},{"cell_type":"code","source":["day_ahead_prices_ = get_DataFrame_from_SQL_database(cur_nonull_, 'DAY_AHEAD_PRICES', startDate_formatted_, endDate_formatted_, nullvalues = True)\n","\n","day_ahead_prices_with_weekday_ = get_weekday(day_ahead_prices_, 'date')\n","\n","day_ahead_prices_with_weekday_['YEAR'] = [int(string[:4]) for string in day_ahead_prices_with_weekday_['date']]\n","\n","new_col_ = get_previous_value_based_on_weekdays(day_ahead_prices_with_weekday_['weekday'].values, day_ahead_prices_with_weekday_['priceEur'].values)\n","\n","day_ahead_prc_df_ = drop_col_and_add_in_df(day_ahead_prices_with_weekday_, ['date', 'priceEur', 'weekday'], {'DA_PRC_DAY_BEFORE':new_col_})\n","\n","day_ahead_prc_df_[f'DA_AV_PRC_{days_to_take_average_over_}_DAYS'] = get_average_over_x_days_based_on_weekdays( day_ahead_prc_df_['weekday'].values, day_ahead_prices_with_weekday_['priceEur'].values, days_to_take_average_over_)\n","\n","# einbauen von Dummy Variablen für Werktage / Wochentage\n","day_ahead_prc_df_.loc[day_ahead_prc_df_['WEEKDAY'].isin([5, 6]), 'DUMMY_WEEKDAY'] = 2\n","day_ahead_prc_df_.loc[day_ahead_prc_df_['WEEKDAY'].isin([0, 1, 2, 3, 4]), 'DUMMY_WEEKDAY'] = 1\n","\n","day_ahead_prc_df_['DA_PRC_SAME_DAY'] = day_ahead_prc_df_['priceEur']\n","# der muss in datenbank\n","day_ahead_prc_df_.drop(columns = [ 'priceEur'], axis = 1, inplace = True)\n","\n","cur_timeseries\n","\n","write_dataframe_to_database(day_ahead_prc_df_, 'DA_PRC_timeseries', conn_timeseries, cur_timeseries)"],"metadata":{"id":"d1t9o0Tcsven"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Volumen, selbes prinzip wie bei Preisen"],"metadata":{"id":"MjN49dnrIcg8"}},{"cell_type":"code","source":["day_ahead_volume_ = get_DataFrame_from_SQL_database(cur_nonull_, 'DAY_AHEAD_VOLUME', startDate_formatted_, endDate_formatted_, nullvalues = True)\n","\n","day_ahead_volume_with_weekday_ = get_weekday(day_ahead_volume_, 'date')\n","\n","new_col_ = get_previous_value_based_on_weekdays(day_ahead_volume_with_weekday_['weekday'].values, day_ahead_volume_with_weekday_['volumeOfAsk'].values)\n","\n","day_ahead_vol_df_ = drop_col_and_add_in_df(day_ahead_volume_with_weekday_, ['date', 'volumeOfAsk', 'weekday'], {'DA_VOL_DAY_BEFORE':new_col_})\n","\n","day_ahead_vol_df_['DA_VOL_SAME_DAY'] = day_ahead_vol_df_['volumeOfAsk']\n","\n","day_ahead_vol_df_[f'DA_AV_VOL_{days_to_take_average_over_}_DAYS'] = get_average_over_x_days_based_on_weekdays( day_ahead_vol_df_['weekday'].values, day_ahead_vol_df_['DA_VOL_SAME_DAY'].values, days_to_take_average_over_)\n","# der muss in datenbank\n","day_ahead_vol_df_.drop(columns = ['weekday', 'volumeOfAsk'], axis = 1, inplace = True)\n","\n","write_dataframe_to_database(day_ahead_vol_df_, 'DA_VOL_timeseries', conn_timeseries, cur_timeseries)"],"metadata":{"id":"Y8YWtOUmyvAc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1715780158481,"user_tz":-120,"elapsed":689,"user":{"displayName":"Alex Krause","userId":"03640757157319878917"}},"outputId":"b0cea9e6-6148-451c-f4ab-e317c357f2f9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["DA_VOL_timeseries Table now in database, database not closed, please close later\n"]}]},{"cell_type":"markdown","source":["Geplante Produktion, hier wollen wir einmal die geplante Produktion für die jeweilige Stunde haben und dann im zweiten schritt die tatsächliche Produktion mit der geplanten Vergleichen bis X (3, 2, oder 1) vor Produktstart"],"metadata":{"id":"SjQ-RlGkL40O"}},{"cell_type":"code","source":["planned_production_ = get_DataFrame_from_SQL_database(cur_nonull_, 'PLANNED_PRODUCTION', startDate_formatted_, endDate_formatted_, nullvalues = True)\n","\n","planned_production_ = planned_production_.rename(columns = {'toplam': 'PL_PR_TOTAL',\t'dogalgaz':'PL_PR_GAS',\t'ruzgar':'PL_PR_WIND',\t'linyit':'PL_PR_LIGNITE',\t'tasKomur':'PL_PR_BC',\t'ithalKomur':'PL_PR_IC',\n","                                                            'fuelOil':'PL_PR_OIL',\t'jeotermal':'PL_PR_GEO',\t'barajli':'PL_PR_HYDRO',\t'nafta':'PL_PR_NAPHTA',\t'biokutle':'PL_PR_BIO',\t'akarsu':'PL_PR_RIVER',\t'diger':'PL_PR_OTHER'})\n","\n","planned_production_.drop(columns = ['time'], axis = 1, inplace = True)\n","\n","# der muss in datenbank\n","cur_timeseries('DROP TABLE PLANNED_PR_timeseries')\n","\n","write_dataframe_to_database(planned_production_, 'PLANNED_PR_timeseries', conn_timeseries, cur_timeseries)"],"metadata":{"id":"fM2cXhb8L3r0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1715780193262,"user_tz":-120,"elapsed":1034,"user":{"displayName":"Alex Krause","userId":"03640757157319878917"}},"outputId":"6b61bcbe-2f59-41d9-f99f-bc9a033b7ccd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["PLANNED_PR_timeseries Table now in database, database not closed, please close later\n"]}]},{"cell_type":"code","source":["# hier wird der Unterschied zwischen der tatsächlichen produktion und der geplanten Produktion ermittelt. Hintergrund ist, dass es ein Zeichen sein könnte\n","# wenn diese beiden größen stark abweichen, der Forecast nicht gestimmt hat und deswegen auch für die folgestunden dieser evtl. nicht stimmen könnte\n","\n","injection_quantitiy_ = get_DataFrame_from_SQL_database(cur_nonull_, 'INJECTION_QUANTITY', startDate_formatted_, endDate_formatted_, nullvalues = True)\n","\n","injection_quantitiy_ = injection_quantitiy_.rename(columns = {'total':'INJ_TOTAL',\t'naturalGas':'INJ_GAS','dam':'INJ_HYDRO',\t'lignite':'INJ_LIGNITE',\t'river':'INJ_RIVER', 'importedCoal':'INJ_IC', 'sun':'INJ_SUN',\t'wind':'INJ_WIND',\t'fueloil':'INJ_OIL',\t'geothermal':'INJ_GEO',\n","                                       'asphaltite':'INJ_ASPHAL', 'stoneCoal':'INJ_BC',\t'biomass':'INJ_BIO',\t'naphtha':'INJ_NAPHTA',\t'lng':'INJ_LNG',\t'internationalImport':'INJ_II',\t'internationalExport':'INJ_IE',\t'other':'INJ_OTHER'})\n","\n","injection_quantitiy_ = injection_quantitiy_.drop_duplicates(subset='date')\n","injection_quantitiy_ = injection_quantitiy_[:26280]\n","\n","type_prod_ = ['TOTAL', 'GAS', 'WIND', 'LIGNITE', 'BC', 'IC', 'OIL', 'GEO', 'HYDRO', 'NAPHTA', 'BIO', 'RIVER', 'OTHER']\n","\n","only_inj_ = ['INJ_SUN', 'INJ_ASPHAL', 'INJ_LNG', 'INJ_II', 'INJ_IE']\n","\n","differenz_df_prod_inj = injection_quantitiy_.copy()\n","differenz_df_prod_inj = differenz_df_prod_inj.reset_index(drop = True)\n","\n","cols_to_shift = injection_quantitiy_.columns.difference(['date'])\n","for hours_before_product in range(1, 4):\n","  injection_quantitiy_shifted_ = injection_quantitiy_[cols_to_shift].shift(hours_before_product)\n","\n","  injection_quantitiy_shifted_['date'] = injection_quantitiy_['date']\n","\n","  for only_inj in only_inj_:\n","    differenz_df_prod_inj[only_inj+'_'+str(hours_before_product)+'_h'] = injection_quantitiy_shifted_[only_inj]\n","\n","    try:\n","      differenz_df_prod_inj.drop(columns = [only_inj], axis = 1, inplace = True)\n","    except:\n","      pass\n","\n","  for typ in type_prod_:\n","    differenz_df_prod_inj[f'DIFFERENZ_PR_INJ_{typ}_{hours_before_product}_h'] = injection_quantitiy_shifted_[f'INJ_{typ}'] - planned_production_[f'PL_PR_{typ}']\n","\n","    try:\n","      differenz_df_prod_inj.drop(columns = [f'INJ_{typ}'], axis = 1, inplace = True)\n","    except:\n","      pass\n","# der muss in datenbank\n","# differenz_df_prod_inj.drop(columns = ['hour'], axis = 1, inplace = True)\n","\n","write_dataframe_to_database(differenz_df_prod_inj, 'PROD_INJ_timeseries', conn_timeseries, cur_timeseries)\n","\n","# das hier sind die durchschnittlichen abweichungen der 3 stunden zuvor für die einzelnen produkte\n","differenz_df_prod_inj_rolling_window_ = differenz_df_prod_inj.drop(columns = ['date', 'hour']).rolling(window=3).mean()\n","differenz_df_prod_inj_rolling_window_['date'] = differenz_df_prod_inj['date']\n","\n","differenz_df_prod_inj_rolling_window_ = differenz_df_prod_inj_rolling_window_.rename(columns={col: 'ROLLING_WINDOW_' + col for col in differenz_df_prod_inj_rolling_window_.columns})\n","differenz_df_prod_inj_rolling_window_ = differenz_df_prod_inj_rolling_window_.rename(columns = {'ROLLING_WINDOW_date':'date'})\n","\n","# der muss in datenbank\n","#differenz_df_prod_inj_rolling_window_['date'] = differenz_df_prod_inj['date']\n","\n","write_dataframe_to_database(differenz_df_prod_inj_rolling_window_, 'PROD_INJ_RW_timeseries', conn_timeseries, cur_timeseries)"],"metadata":{"id":"gg1dOcdpO9-U","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1715780212487,"user_tz":-120,"elapsed":2611,"user":{"displayName":"Alex Krause","userId":"03640757157319878917"}},"outputId":"37b49ae2-5644-40b8-f087-b85fd6cbc2fd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["PROD_INJ_timeseries Table now in database, database not closed, please close later\n","PROD_INJ_RW_timeseries Table now in database, database not closed, please close later\n"]}]},{"cell_type":"markdown","source":["Das selbe mit forecast und real load"],"metadata":{"id":"RvgcSlNGaoSp"}},{"cell_type":"code","source":["forecast_load_ =get_DataFrame_from_SQL_database(cur_nonull_, 'FORECAST_LOAD', startDate_formatted_, endDate_formatted_, nullvalues = True)\n","# der muss in datenbank\n","forecast_load_= forecast_load_.rename(columns = {'lep':'FORECAST_LOAD_SAME_DAY'})\n","forecast_load_.drop(columns = ['time'], axis = 1, inplace = True)\n","\n","write_dataframe_to_database(forecast_load_, 'FOR_LOAD_timeseries', conn_timeseries, cur_timeseries)\n","\n","real_load_ = get_DataFrame_from_SQL_database(cur_nonull_, 'REAL_TIME_CONSUMPTION', startDate_formatted_, endDate_formatted_, nullvalues = True)"],"metadata":{"id":"4S5oSsviRzSD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1715780222742,"user_tz":-120,"elapsed":769,"user":{"displayName":"Alex Krause","userId":"03640757157319878917"}},"outputId":"8f30a588-937f-4b14-f9e6-a9cfba6348c8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["FOR_LOAD_timeseries Table now in database, database not closed, please close later\n"]}]},{"cell_type":"code","source":["differenz_for_real_load_ = real_load_.copy()\n","differenz_for_real_load_.drop(columns = ['consumption'], axis = 1, inplace = True)\n","for hours_before_product in range(1, 4):\n","  real_load_shifted_ = real_load_['consumption'].shift(hours_before_product)\n","  # der muss in datenbank\n","  differenz_for_real_load_[f'DIFFERENZ_FOR_REAL_LOAD_{hours_before_product}_h'] = real_load_shifted_ - forecast_load_['FORECAST_LOAD_SAME_DAY']\n","\n","write_dataframe_to_database(differenz_for_real_load_, 'FOR_REAL_LOAD_timeseries', conn_timeseries, cur_timeseries)\n","\n","differenz_for_real_load_rolling_window_ = differenz_for_real_load_.drop(columns = ['date', 'time']).rolling(window=window_size_rolling_window).mean()\n","differenz_for_real_load_rolling_window_ = differenz_for_real_load_rolling_window_.rename(columns={col: f'ROLLING_WINDOW_PAST_{window_size_rolling_window}_h_' + col for col in differenz_for_real_load_rolling_window_.columns})\n","\n","# der muss in datenbank\n","differenz_for_real_load_rolling_window_['date'] = differenz_for_real_load_['date']\n","\n","write_dataframe_to_database(differenz_for_real_load_rolling_window_, 'FOR_REAL_LOAD_RW_timeseries', conn_timeseries, cur_timeseries)"],"metadata":{"id":"hU74_5qIZx1q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1715780225630,"user_tz":-120,"elapsed":830,"user":{"displayName":"Alex Krause","userId":"03640757157319878917"}},"outputId":"19b678fa-d92e-4ac6-954a-756dca9fdb30"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["FOR_REAL_LOAD_timeseries Table now in database, database not closed, please close later\n","FOR_REAL_LOAD_RW_timeseries Table now in database, database not closed, please close later\n"]}]},{"cell_type":"markdown","source":["System direction"],"metadata":{"id":"bmCIav5RsN2j"}},{"cell_type":"code","source":["system_direction_ = get_DataFrame_from_SQL_database(cur_nonull_, 'SYSTEM_DIRECTION', startDate_formatted_, endDate_formatted_, nullvalues = True)\n","for hour in range(1,4):\n","  system_direction_[f'SYSTEM_DIRECTION_{hour}_h'] = system_direction_['smpDirectionId'].shift(hour)\n","\n","  # get rolling windows\n","  system_direction_[f'ROLLING_WINDOW_PAST_{window_size_rolling_window}_h_'+f'SYSTEM_DIRECTION_{hour}_h'] = system_direction_[f'SYSTEM_DIRECTION_{hour}_h'].rolling(window=window_size_rolling_window).mean()\n","\n","system_direction_.drop(columns = ['hour', 'systemDirection', 'smpDirectionId'], axis = 1, inplace = True)\n","\n","write_dataframe_to_database(system_direction_, 'SYSTEM_DIRECTION_timeseries', conn_timeseries, cur_timeseries)"],"metadata":{"id":"4CpSFww0T_Q-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1715780241390,"user_tz":-120,"elapsed":709,"user":{"displayName":"Alex Krause","userId":"03640757157319878917"}},"outputId":"84e77ec4-0bb5-462d-b33b-f1978c30c86d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["SYSTEM_DIRECTION_timeseries Table now in database, database not closed, please close later\n"]}]},{"cell_type":"markdown","source":["Up & Down Regulation"],"metadata":{"id":"AZxCPyPIvhhj"}},{"cell_type":"code","source":["up_regulation_ = get_DataFrame_from_SQL_database(cur_nonull_, 'UP_REGULATION', startDate_formatted_, endDate_formatted_, nullvalues = True)\n","\n","up_regulation_.date = [i[:-3] + i[-2:] for i in up_regulation.date]\n","\n","for hour in range(1,4):\n","  up_regulation_[f'UP_REG_{hour}_h_0'] =  up_regulation_['upRegulationZeroCoded'].shift(hour)\n","  up_regulation_[f'UP_REG_{hour}_h_1'] =  up_regulation_['upRegulationOneCoded'].shift(hour)\n","  up_regulation_[f'UP_REG_{hour}_h_2'] =  up_regulation_['upRegulationTwoCoded'].shift(hour)\n","\n","up_regulation_.drop(columns = ['hour','upRegulationZeroCoded','upRegulationOneCoded', 'upRegulationTwoCoded', 'upRegulationDelivered', 'net' ], axis = 1, inplace = True)\n","\n","write_dataframe_to_database(up_regulation_, 'UP_REG_timeseries', conn_timeseries, cur_timeseries)"],"metadata":{"id":"9uHVdhJoO4OC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1715780246891,"user_tz":-120,"elapsed":800,"user":{"displayName":"Alex Krause","userId":"03640757157319878917"}},"outputId":"4341cd9b-3928-4538-a89d-1c09abb7c520"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["UP_REG_timeseries Table now in database, database not closed, please close later\n"]}]},{"cell_type":"code","source":["DOWN_regulation_ = get_DataFrame_from_SQL_database(cur_nonull_, 'DOWN_REGULATION', startDate_formatted_, endDate_formatted_, nullvalues = True)\n","\n","DOWN_regulation_.date = [i[:-3] + i[-2:] for i in DOWN_regulation_.date]\n","\n","for hour in range(1,4):\n","  DOWN_regulation_[f'DOWN_REG_{hour}_h_0'] =  DOWN_regulation_['downRegulationZeroCoded'].shift(hour)\n","  DOWN_regulation_[f'DOWN_REG_{hour}_h_1'] =  DOWN_regulation_['downRegulationOneCoded'].shift(hour)\n","  DOWN_regulation_[f'DOWN_REG_{hour}_h_2'] =  DOWN_regulation_['downRegulationTwoCoded'].shift(hour)\n","\n","DOWN_regulation_.drop(columns = ['hour','downRegulationZeroCoded','downRegulationOneCoded', 'downRegulationTwoCoded', 'downRegulationDelivered', 'net' ], axis = 1, inplace = True)\n","\n","DOWN_regulation_ = DOWN_regulation_.drop_duplicates()[:26280]\n","\n","write_dataframe_to_database(DOWN_regulation_, 'DOWN_REG_timeseries', conn_timeseries, cur_timeseries)"],"metadata":{"id":"hogLHeVGvsSc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1715780247378,"user_tz":-120,"elapsed":488,"user":{"displayName":"Alex Krause","userId":"03640757157319878917"}},"outputId":"f0d0af65-63ad-4478-8d92-ddfd451f858e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["DOWN_REG_timeseries Table now in database, database not closed, please close later\n"]}]},{"cell_type":"markdown","source":["Failure Information"],"metadata":{"id":"gf9kEYdW7qGu"}},{"cell_type":"code","source":["\n","\n","def create_new_rows_failure_data(row, col_name, name_hourly = 'hourlyLoadAvg', name_start = 'startTime', name_end = 'endTime'):\n","  \"\"\"\n","  Funktion die folgendes macht:\n","  aus dem Datensatz\n","\n","  start                     end                   hourlyAvg\n","  09.01.2022 11:20 Uhr      09.01.2022 13:40      10\n","\n","  macht sie den datensatz\n","  date                  col_name\n","  09.01.2022 11 Uhr     40/60 * 10\n","  09.01.2022 12 Uhr     60/60 * 10\n","  09.01.2022 13 Uhr     40/60 * 10\n","\n","  D.h. sie berechnet den anteiligen hourlyAvg auf die Stunden und gibt dies als neue Zeilen aus\n","  \"\"\"\n","  # some weird error with tab character, fixed this\n","  correct_column_name_start = name_start.rstrip('\\t')\n","  correct_column_name_end = name_end.rstrip('\\t')\n","\n","  start_time = pd.to_datetime(row[correct_column_name_start])\n","  end_time = pd.to_datetime(row[correct_column_name_end])\n","  hourly_avg = row[name_hourly]\n","\n","\n","  end_hour = end_time.hour\n","  start_hour = start_time.hour\n","\n","  end_minute = end_time.minute\n","  start_minute = start_time.minute\n","\n","  if end_minute == 0:\n","    # wenn der ausfall um 11:00 endet, dann zählt er nicht mehr für stunde 11-12\n","    end_hour = end_hour - 1\n","\n","  # Erstellen Sie neue Zeilen für jede beeinflusste Stunde\n","  new_rows = []\n","  new_start = start_time - timedelta(minutes = start_time.minute)\n","\n","  if end_hour == start_hour:\n","    h_load_avg = (end_minute - start_minute)/ 60 * hourly_avg\n","    new_rows.append({'date':new_start.strftime('%Y-%m-%d %H:%M:%S%z'), col_name:h_load_avg})\n","  else:\n","    # more than one hour affected\n","    for hour in range(start_hour, end_hour+1):\n","      # check if we are in start_hour\n","      if hour == start_hour:\n","        h_load_avg = (60 - start_minute)/ 60 * hourly_avg # dann haben wir auf jeden fall volle stunde zur verfügung, da wir oben den fall ausschließen, dass wir gleich end_hour sind\n","        new_rows.append({'date':new_start.strftime('%Y-%m-%d %H:%M:%S%z'), col_name:h_load_avg})\n","\n","      elif hour == end_hour: # hier sind wir gleich der endstunde, d.h. wir müssen wieder auf die minuten gucken\n","\n","        if end_minute == 0:\n","          h_load_avg = hourly_avg\n","        else:\n","          h_load_avg = (end_minute)/ 60 * hourly_avg\n","\n","\n","        new_rows.append({'date':new_start.strftime('%Y-%m-%d %H:%M:%S%z'), col_name:h_load_avg})\n","\n","      else:\n","        h_load_avg = hourly_avg\n","\n","        new_rows.append({'date':new_start.strftime('%Y-%m-%d %H:%M:%S%z'), col_name:h_load_avg})\n","\n","      # setze die stunde hoch\n","      new_start = new_start + timedelta(hours = 1)\n","\n","    return new_rows"],"metadata":{"id":"z01SlhnSjmw8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","def split_failure_data(failure_df, col_name, name_hourly = 'hourlyLoadAvg', name_start = 'startTime', name_end = 'endTime'):\n","  \"\"\"\n","  Diese Funktion aggregiert dei Failure Informationen nach der Stunde auf mit Hilfe\n","  der split Funktion create_new_rows_failure_data\n","\n","  \"\"\"\n","  failure_df_without_0 = failure_df[failure_df[name_hourly]!=0]\n","\n","  new_failure_df = pd.DataFrame(columns = ['date', col_name])\n","\n","  for i in tqdm(range(len(failure_df_without_0))):\n","    new_rows = create_new_rows_failure_data(failure_df_without_0.iloc[i,:], col_name, name_hourly = name_hourly, name_start = name_start, name_end = name_end)\n","\n","    new_failure_df = pd.concat([new_failure_df, pd.DataFrame(new_rows)])\n","\n","  return new_failure_df.groupby('date').sum()\n"],"metadata":{"id":"Jei4hyGOmbXc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# planned failure df\n","\n","planned_failure_df_ = get_DataFrame_from_SQL_database(cur_nonull_, 'PLANNED_FAILURE', startDate_formatted_, endDate_formatted_, nullvalues = True)\n","splitted_failure_load_df_ = split_failure_data(planned_failure_df_, 'PL_FI_LOAD')\n","\n","index = pd.date_range(start='2021-01-01 00:00:00', end='2023-12-31 23:00:00', freq='H', tz='Etc/GMT-3')\n","full_splitted_failure_load_df_ = pd.DataFrame(index=index, columns = ['PL_FI_LOAD'])\n","splitted_failure_load_df_.index = pd.to_datetime(splitted_failure_load_df_.index)\n","\n","# der muss in datenbank\n","full_splitted_failure_load_df_.update(splitted_failure_load_df_)\n","\n","full_splitted_failure_load_df_['date'] = full_splitted_failure_load_df_.index.strftime('%Y-%m-%dT%H:%M:%S%z')\n","\n","full_splitted_failure_load_df_ = full_splitted_failure_load_df_.reset_index(drop= True)\n","\n","full_splitted_failure_load_df_ = full_splitted_failure_load_df_.fillna(0)\n","\n","write_dataframe_to_database(full_splitted_failure_load_df_, 'FI_PL_LOAD_timeseries', conn_timeseries, cur_timeseries)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fd4Q0KcpzyfX","executionInfo":{"status":"ok","timestamp":1715780271907,"user_tz":-120,"elapsed":568,"user":{"displayName":"Alex Krause","userId":"03640757157319878917"}},"outputId":"03f8c5a5-662f-4698-c8d5-dfdf7e9c98e5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 7/7 [00:00<00:00, 203.63it/s]\n"]},{"output_type":"stream","name":"stdout","text":["FI_PL_LOAD_timeseries Table now in database, database not closed, please close later\n"]}]},{"cell_type":"code","source":["# unplanned failure information load\n","\n","unplanned_failure_df_ = get_DataFrame_from_SQL_database(cur_nonull_, 'UNPLANNED_FAILURE', startDate_formatted_, endDate_formatted_, nullvalues = True)\n","splitted_unplanned_failure_load_df_ = split_failure_data(unplanned_failure_df_, 'UNPL_FI_LOAD')\n","\n","index = pd.date_range(start='2021-01-01 00:00:00', end='2023-12-31 23:00:00', freq='H', tz='Etc/GMT-3')\n","full_splitted_unpl_failure_load_df_ = pd.DataFrame(index=index, columns = ['UNPL_FI_LOAD'])\n","splitted_unplanned_failure_load_df_.index = pd.to_datetime(splitted_unplanned_failure_load_df_.index)\n","\n","full_splitted_unpl_failure_load_df_.update(splitted_unplanned_failure_load_df_)\n","\n","full_splitted_unpl_failure_load_df_['date'] = full_splitted_unpl_failure_load_df_.index.strftime('%Y-%m-%dT%H:%M:%S%z')\n","\n","full_splitted_unpl_failure_load_df_ = full_splitted_unpl_failure_load_df_.fillna(0)\n","\n","for hour in range(1,4):\n","  full_splitted_unpl_failure_load_df_[f'UNPL_FI_LOAD_{hour}_h'] = full_splitted_unpl_failure_load_df_['UNPL_FI_LOAD'].shift(hour)\n","\n","  full_splitted_unpl_failure_load_df_[f'ROLLING_WINDOW_PAST_{window_size_rolling_window}_h_'+f'UNPL_FI_LOAD_{hour}_h'] = full_splitted_unpl_failure_load_df_[f'UNPL_FI_LOAD_{hour}_h'].rolling(window=window_size_rolling_window).mean()\n","\n","# der muss in datenbank\n","full_splitted_unpl_failure_load_df_ = full_splitted_unpl_failure_load_df_.reset_index(drop= True)\n","full_splitted_unpl_failure_load_df_.drop(columns = ['UNPL_FI_LOAD'], axis = 1, inplace = True)\n","\n","write_dataframe_to_database(full_splitted_unpl_failure_load_df_, 'FI_UNPL_LOAD_timeseries', conn_timeseries, cur_timeseries)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d6TTQWoQ2DW3","executionInfo":{"status":"ok","timestamp":1715780316767,"user_tz":-120,"elapsed":5217,"user":{"displayName":"Alex Krause","userId":"03640757157319878917"}},"outputId":"030976e4-2d5a-44fc-f3e1-6669f85b414a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 1109/1109 [00:04<00:00, 249.66it/s]\n"]},{"output_type":"stream","name":"stdout","text":["FI_UNPL_LOAD_timeseries Table now in database, database not closed, please close later\n"]}]},{"cell_type":"markdown","source":["POWER INFORMATION FAILURE"],"metadata":{"id":"YrMsVUhilYqS"}},{"cell_type":"code","source":["failure_message_df_ = get_DataFrame_from_SQL_database(cur_nonull_, 'FAILURE_MESSAGE', startDate_formatted_, endDate_formatted_, nullvalues = True)\n","failure_message_df_['DIFF_CAP'] = failure_message_df_['capacityAtCaseTime'] - failure_message_df_['operatorPower']\n","\n","splitted_power_failure_ = split_failure_data(failure_message_df_, col_name = 'FAILURE_CAP', name_hourly = 'DIFF_CAP', name_start='caseStartDate\t', name_end= 'caseEndDate' )\n","\n","for hour in range(1,4):\n","  splitted_power_failure_[f'POWER_FI_{hour}_h'] = splitted_power_failure_['FAILURE_CAP'].shift(hour)\n","\n","  splitted_power_failure_[f'ROLLING_WINDOW_PAST_{window_size_rolling_window}_h_'+f'POWER_FI_{hour}_h'] = splitted_power_failure_[f'POWER_FI_{hour}_h'].rolling(window=window_size_rolling_window).mean()\n","\n","splitted_power_failure_['date'] = pd.to_datetime(splitted_power_failure_.index).strftime('%Y-%m-%dT%H:%M:%S%z')\n","splitted_power_failure_.drop(columns = ['FAILURE_CAP'], axis = 1, inplace = True)\n","splitted_power_failure_ = splitted_power_failure_.reset_index(drop = True)\n","\n","write_dataframe_to_database(splitted_power_failure_, 'FI_POWER_timeseries', conn_timeseries, cur_timeseries)"],"metadata":{"id":"Z1thlHTPlckX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1715782367703,"user_tz":-120,"elapsed":1688012,"user":{"displayName":"Alex Krause","userId":"03640757157319878917"}},"outputId":"f924c482-dc85-45cd-d89f-84ed1e124592"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 93131/93131 [28:04<00:00, 55.28it/s]\n"]},{"output_type":"stream","name":"stdout","text":["FI_POWER_timeseries Table now in database, database not closed, please close later\n"]}]},{"cell_type":"markdown","source":["WPP DATA"],"metadata":{"id":"DyWHBvKTvbGd"}},{"cell_type":"markdown","source":["wir wollen für jede stunde folgende Datensätze:\n","Produktion Stunde davor Forecast Q1   Forecast Q2   Forecast Q3   Forecast Q4 (Forecasts immer für die selbe Stunde)\n","\n","wir müssen hier noch die shifts richtig machen, sodass wir basierednd auf der shift number folgende featrues erhalten:\n","\n","generation_3_h_vorher\n","\n","differenz_generation_for_3_h_vorher\n","\n","generation_4_h_vorher\n","\n","differenz_generation_for_4_h_vorher\n","\n","generation_5_h_vorher\n","\n","differenz_generation_for_5_h_vorher\n","\n","(hier ist shift number 3, sonst fange bei 2 an)\n"],"metadata":{"id":"wzRipoVs6wyj"}},{"cell_type":"code","source":["def agg_wpp(df):\n","  \"\"\"\n","  Funktion um das Datenset aus den Wind Prognosen zu aggregieren.\n","  Hier wird das 10 Min Raster zu stunden raster aggregiert, Datensätze sehen dann wie folgt aus\n","\n","  Datum               Forecast 11 Uhr     Mean Pred. Q1 11 Uhr    Mean Pred Q2 11 Mean Pred Q3 11  Mean Pred Q4 11 GENERATIOn 11 Uhr\n","  10.01.2022 11 Uhr   1000\n","  \"\"\"\n","  df['datetime'] = pd.to_datetime(df['date'])\n","\n","  df['date_hour'] = df['datetime'].dt.floor('H')\n","\n","  df_agg = df.groupby('date_hour').agg({'generation': 'sum','forecast':'sum', 'quarter1': 'mean', 'quarter2': 'mean', 'quarter3': 'mean', 'quarter4': 'mean'})\n","\n","  return df_agg\n","\n","def shift_wpp(df_agg, columnFORQ1 = 'FORECAST_WPP_Q1', columnFORQ2 = 'FORECAST_WPP_Q2', columnFORQ3 = 'FORECAST_WPP_Q3', columnFORQ4 = 'FORECAST_WPP_Q4',\n","                       columnGENShifted = 'GENERATION_WPP', columnFORECAST = 'FORECAST_WPP', columnDiffForGEN = 'DIFF_FOR_GEN_WPP', shift_num = 3, vorherige_h = 3):\n","  '''\n","  Input ist df_agg von WPP\n","\n","  Dann erstellt die Funktion daraus folgende Daten für den 10.01.2022 11 Uhr\n","\n","  date                forecast 11 Uhr   forecast Q1 8 Uhr   forecast Q2 8 Uhr .. DIFF_FORECAST_GEN 7-8 Uhr | DIFF_FORECAST_GEN 6-7 | DIFF_FORECAST_GEN 5-6 | GENERATION 8 Uhr\n","  10.01.2022 11 Uhr   1000\n","\n","  '''\n","  df_agg[columnDiffForGEN] = df_agg['generation'] - df_agg['forecast']\n","\n","  for shift in range(1, shift_num+1):\n","    col_name_diff = columnDiffForGEN + f'_{shift}_h'\n","    col_name_gen = columnGENShifted + f'_{shift}_h'\n","\n","    df_agg[col_name_diff] = get_shifted_df_different_shifts(df_agg, columnDiffForGEN, shift)\n","    df_agg[col_name_gen] = get_shifted_df_different_shifts(df_agg, 'generation', shift)\n","\n","\n","  df_agg[columnFORQ1] = df_agg['quarter1']\n","  df_agg[columnFORQ2] = df_agg['quarter2']\n","  df_agg[columnFORQ3] = df_agg['quarter3']\n","  df_agg[columnFORQ4] = df_agg['quarter4']\n","\n","  df_agg[columnFORECAST] = df_agg['forecast']\n","\n","  df_final = df_agg.drop(columns = ['forecast', 'generation', 'quarter1', 'quarter2', 'quarter3', 'quarter4'])\n","\n","  df_final['date'] = df_final.index.strftime('%Y-%m-%dT%H:%M:%S%z')\n","\n","  df_final.drop(columns = ['DIFF_FOR_GEN_WPP'], axis = 1, inplace = True)\n","\n","  df_final = df_final.reset_index(drop = True)\n","\n","  return df_final"],"metadata":{"id":"_BlzIvDt6wEl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["wpp_gen_forecast_df_ = get_DataFrame_from_SQL_database(cur_nonull_, 'WPP_GENERATION_AND_FORECAST', startDate_formatted_, endDate_formatted_, nullvalues = True)\n","\n","agg_wpp_gen_forecast_df_ = agg_wpp(wpp_gen_forecast_df_)\n","\n","final_wpp_ = shift_wpp(agg_wpp_gen_forecast_df_)\n","\n","write_dataframe_to_database(final_wpp_, 'WPP_timeseries', conn_timeseries, cur_timeseries)"],"metadata":{"id":"iIWbNFgF8FYs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1715782481632,"user_tz":-120,"elapsed":3868,"user":{"displayName":"Alex Krause","userId":"03640757157319878917"}},"outputId":"770c0387-7674-4509-dd88-24712666fa65"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["WPP_timeseries Table now in database, database not closed, please close later\n"]}]},{"cell_type":"markdown","source":["Trading daten"],"metadata":{"id":"ze6oLFuEvFlU"}},{"cell_type":"code","source":["\n","\n","def group_trading_data(trading_df, hour_before):\n","  \"\"\"\n","  Funktion die uns pro Produkt folgende Daten zurückgibt:\n","  contractName  AbstandTrading\t                                      TRADE_VOLUME                                       TRADE_COUNT\t    GewichteterPreis\n","  PH21010108\t  (>0, wenn in der letzten Stunde vor Produkt gekauft,  insgesamt gehandeltes >TRADE_VOLUMEn in dem zeitraum    anzahl Trades   preis * menge / menge\n","                  >1, wenn 1-2 Stunden vorher\n","                  >2 wenn 2-3 Stunden vorhre\n","                  >3 wenn 3- ende offen stunden vorher)\n","\n","\n","  \"\"\"\n","  # schmeisse Viertelstundenprodukte raus\n","  trading_df = trading_df[trading_df.loc[:,'contractName'].str.startswith('PH')]\n","\n","  #trading_df['contractDate'] = trading_df['contractName'].apply(lambda x: datetime.strptime(x[2:], '%y%m%d%H').replace(tzinfo=pytz.FixedOffset(180)))\n","  trading_df.loc[:, 'contractDate'] = trading_df.loc[:,'contractName'].apply(lambda x: datetime.strptime(x[2:], '%y%m%d%H').replace(tzinfo=pytz.FixedOffset(180)))\n","\n","\n","  trading_df.loc[:,'datetime'] = pd.to_datetime(trading_df.loc[:,'date'])\n","\n","  # Differenz zwischen 'contractDate' und 'date' berechnen und 'AbstandTrading' erstellen\n","  if hour_before == 3:\n","    trading_df.loc[:,'AbstandTrading'] = trading_df.apply(lambda row: '>3' if (row['contractDate'] - row['datetime']).total_seconds() / 3600 > 3\n","                                    else '>2' if 2 < (row['contractDate'] - row['datetime']).total_seconds() / 3600 <= 3\n","                                    else '>1' if 1 < (row['contractDate'] - row['datetime']).total_seconds() / 3600 <= 2\n","                                    else '>=0', axis=1)\n","  elif hour_before == 2:\n","    trading_df.loc[:,'AbstandTrading'] = trading_df.apply(lambda row:\n","                                   '>2' if 2 < (row['contractDate'] - row['datetime']).total_seconds() / 3600\n","                                    else '>1' if 1 < (row['contractDate'] - row['datetime']).total_seconds() / 3600 <= 2\n","                                    else '>=0', axis=1)\n","  elif hour_before == 1:\n","    trading_df.loc[:,'AbstandTrading'] = trading_df.apply(lambda row:\n","                                    '>1' if 1 < (row['contractDate'] - row['datetime']).total_seconds() / 3600\n","                                    else '>=0', axis=1)\n","\n","  # 'price' und 'quantity' multiplizieren und 'PriceTimesQuantity' erstellen\n","  trading_df.loc[:,'PriceTimesQuantity'] = trading_df.loc[:,'price'] * trading_df.loc[:,'quantity']\n","\n","\n","  # Gruppieren Sie die Daten und berechnen Sie die Summen, die Volatilität und die Anzahl der Trades\n","  grouped_df = trading_df.groupby(['contractName', 'contractDate', 'AbstandTrading']).agg({\n","      'quantity': np.sum,\n","      'PriceTimesQuantity': np.sum,\n","      'price': lambda x: np.sqrt(np.average((x - np.average(x))**2, weights=trading_df.loc[x.index, 'quantity'])),\n","      'contractName': 'size'\n","  })\n","\n","  # Benennen Sie die Spalten um\n","  grouped_df.columns = ['TRADE_VOLUME', 'SumPriceTimesQuantity', 'TRADE_VOLATILITY', 'TRADE_COUNT']\n","\n","  # Reset Index\n","  grouped_df = grouped_df.reset_index()\n","\n","  grouped_df.loc[:,'GewichteterPreis'] = grouped_df.loc[:,'SumPriceTimesQuantity'] / grouped_df.loc[:,'TRADE_VOLUME']\n","\n","  return grouped_df"],"metadata":{"id":"z4HDabpIzFaV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def create_pivot_table(trading_df, hour_before):\n","  df = group_trading_data(trading_df, hour_before)\n","\n","  if hour_before == 3:\n","    df = df[df['AbstandTrading'] == '>3']\n","  elif hour_before ==2:\n","    df['AbstandTrading'] = df['AbstandTrading'].replace('>3','>2')\n","    df = df[df['AbstandTrading'] == '>2']\n","  elif hour_before == 1:\n","    df['AbstandTrading'] = df['AbstandTrading'].replace('>3','>1')\n","    df['AbstandTrading'] = df['AbstandTrading'].replace('>2','>1')\n","\n","    df = df[df['AbstandTrading'] == '>1']\n","  else:\n","    print('Wrong hour_bfore, can only be 1, 2 or 3')\n","\n","  # Erstellen Sie eine Pivot-Tabelle\n","  pivot_df = df.pivot_table(index=['contractName', 'contractDate'],\n","                            columns='AbstandTrading',\n","                            values=['TRADE_VOLUME', 'TRADE_VOLATILITY', 'TRADE_COUNT', 'SumPriceTimesQuantity'],\n","                            fill_value=0)\n","\n","    # Ändern Sie die Spaltennamen\n","  pivot_df.columns = [col + '_' + h.lstrip('>') + '_h' for col, h in pivot_df.columns]\n","\n","  # Reset Index\n","  pivot_df = pivot_df.reset_index()\n","\n","  pivot_df.loc[:,'date'] = pivot_df.loc[:,'contractDate']\n","\n","  pivot_df.loc[:,f'PRE_ID3_{hour_before}_h'] = pivot_df.loc[:,f'SumPriceTimesQuantity_{hour_before}_h']/pivot_df.loc[:,f'TRADE_VOLUME_{hour_before}_h']\n","\n","  pivot_df.drop(columns = ['contractDate', 'contractName', f'SumPriceTimesQuantity_{hour_before}_h'], axis = 1, inplace = True)\n","\n","  pivot_df.index = pivot_df.loc[:,'date']\n","\n","  # da wir nicht für jeden Zeitpunkt einen Trade haben, der hour_before stunden vorher passsiert ist, hinterher aber joinen wollen,\n","  # erstellen wir uns einen dataframe, der die gesamte zeitspane umfasst\n","  index = pd.date_range(start='2021-01-01 00:00:00', end='2023-12-31 23:00:00', freq='H', tz='Etc/GMT-3')\n","\n","  df_for_updating = pd.DataFrame(index = index, columns = pivot_df.columns)\n","  df_for_updating.update(pivot_df)\n","\n","  df_for_updating.loc[:,'date'] = df_for_updating.index.strftime('%Y-%m-%dT%H:%M:%S%z')\n","  df_for_updating.drop(columns = ['date'], axis = 1, inplace = True)\n","\n","  df_for_updating = df_for_updating.fillna(0)\n","\n","  return df_for_updating\n","\n","# hierauf muss ich jetzt meine durchschnittsfunktionen zum beispiel anwenden\n","\n"],"metadata":{"id":"k_LZS7-_6Q0x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Achtung: Dieser Code Abschnitt kann nicht in Colab ausgeführt werden, da die Trading Daten 21. Mio Datensätze umfassen. Wir haben diesen Abschnitt deshalb auf einem lokalen Rechner ausgeführt\n","# und danach die fertige Datenbank wieder hochgeladenn\n","# timeseries_database umfasst also alle Sachen, die in Colab ausgeführt wurden aus diesem Notebook, timeseries_database_finished dann noch zusätzlich die Kennzahlen der Tradingdaten\n","\n","[conn_trading_IDM_, cur_trading_IDM_] = connect_to_database(\"original_data_trading_IDM_Transaction.db\")\n","\n","trading_df_ = get_DataFrame_from_SQL_database(cur_trading_IDM_, 'IDM_TRANSACTION_HISTORY', startDate_formatted_, endDate_formatted_, nullvalues = False)\n","\n","for hour in range(1, 4):\n","  trading_data_without_averages_ = create_pivot_table(trading_df_, hour)\n","\n","  trading_data_without_averages_['weekday'] = trading_data_without_averages_.index.weekday\n","\n","  trading_data_with_averages_ = trading_data_without_averages_.copy()\n","  for col in trading_data_without_averages_.drop(columns = 'weekday').columns:\n","\n","    trading_data_with_averages_['AV_'+col] = get_average_over_x_days_based_on_weekdays(trading_data_without_averages_['weekday'].values, trading_data_without_averages_[col].values, days_to_take_average_over_)\n","\n","    trading_data_with_averages_['ABW_PROZENT_'+col] = (trading_data_with_averages_[col] - trading_data_with_averages_['AV_'+col])/trading_data_with_averages_['AV_'+col]\n","\n","  trading_data_with_averages_['date'] = trading_data_with_averages_.index.strftime('%Y-%m-%dT%H:%M:%S%z')\n","  trading_data_with_averages_ = trading_data_with_averages_.reset_index(drop = True)\n","\n","  write_dataframe_to_database(trading_data_with_averages_, f'TRADING_DATA_{hour}_timeseries', conn_timeseries, cur_timeseries)\n","\n","\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qZLBnIUWfagX","outputId":"a40f16ea-90e0-4b62-f52e-15ddcb1ea8ec"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-6-9e3ebf45d5be>:22: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  trading_df.loc[:, 'contractDate'] = trading_df.loc[:,'contractName'].apply(lambda x: datetime.strptime(x[2:], '%y%m%d%H').replace(tzinfo=pytz.FixedOffset(180)))\n","<ipython-input-6-9e3ebf45d5be>:25: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  trading_df.loc[:,'datetime'] = pd.to_datetime(trading_df.loc[:,'date'])\n"]}]},{"cell_type":"markdown","source":["ID3 Prices"],"metadata":{"id":"y3TeMOd2nnn4"}},{"cell_type":"code","source":["def get_real_ID3_prices(trading_data):\n","  \"\"\"\n","  Funktion die die tatsächlichen ID3 Preise berechnet\n","  \"\"\"\n","  trading_df = trading_data.copy()\n","  # schmeisse Viertelstundenprodukte raus\n","  trading_df = trading_df[trading_df['contractName'].str.startswith('PH')]\n","\n","  trading_df['contractDate'] = trading_df['contractName'].apply(lambda x: datetime.strptime(x[2:], '%y%m%d%H').replace(tzinfo=pytz.FixedOffset(180)))\n","\n","  trading_df['datetime'] = pd.to_datetime(trading_df['date'])\n","\n","  # Differenz zwischen 'contractDate' und 'date' berechnen und 'AbstandTrading' erstellen\n","\n","  trading_df['AbstandTrading'] = trading_df.apply(lambda row: '>3' if (row['contractDate'] - row['datetime']).total_seconds() / 3600 > 3\n","                                    else 'ID3' , axis=1)\n","\n","  trading_df = trading_df[trading_df['AbstandTrading'] == 'ID3']\n","\n","  # 'price' und 'quantity' multiplizieren und 'PriceTimesQuantity' erstellen\n","  trading_df['PriceTimesQuantity'] = trading_df['price'] * trading_df['quantity']\n","\n","\n","  # Gruppieren Sie die Daten und berechnen Sie die Summen, die Volatilität und die Anzahl der Trades\n","  grouped_df = trading_df.groupby(['contractDate']).agg({\n","      'quantity': np.sum,\n","      'PriceTimesQuantity': np.sum,\n","      'price': lambda x: np.sqrt(np.average((x - np.average(x))**2, weights=trading_df.loc[x.index, 'quantity'])),\n","      'contractName': 'size'\n","  })\n","\n","\n","  # Benennen Sie die Spalten um\n","  grouped_df.columns = ['TRADE_VOLUME', 'SumPriceTimesQuantity', 'TRADE_VOLATILITY', 'TRADE_COUNT']\n","\n","  # Reset Index\n","  #grouped_df = grouped_df.reset_index(drop= True)\n","\n","\n","  grouped_df['ID3'] = grouped_df['SumPriceTimesQuantity'] / grouped_df['TRADE_VOLUME']\n","\n","  index = pd.date_range(start='2021-01-01 00:00:00', end='2023-12-31 23:00:00', freq='H', tz='Etc/GMT-3')\n","\n","  df_for_updating = pd.DataFrame(index = index, columns = grouped_df.columns)\n","  df_for_updating.update(grouped_df)\n","\n","  return df_for_updating\n"],"metadata":{"id":"DxFp736ITte_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ersettze jetzt noch jeden eintrag, der ein NaN hat, mit dem dazugehörigen gewichteten Preis\n","\n","ID3_info_df_ = get_real_ID3_prices(trading_df_)\n","\n","weighted_average_prices_ = get_DataFrame_from_SQL_database(cur_nonull_, 'WEIGHTED_AVERAGE_PRICES', startDate_formatted_, endDate_formatted_, nullvalues = True)\n","weighted_average_prices_.index = pd.to_datetime(weighted_average_prices_['date'])\n","\n","ID3_info_df_['ID3_filled'] =  ID3_info_df_['ID3'].fillna(weighted_average_prices_['wap'])\n","\n","ID3_info_df_['date'] = ID3_info_df_.index.strftime('%Y-%m-%dT%H:%M:%S%z')\n","ID3_info_df_ = ID3_info_df_.reset_index(drop = True)\n","\n","write_dataframe_to_database(ID3_info_df_, 'ID_PRICES_timeseries', conn_timeseries, cur_timeseries)"],"metadata":{"id":"vYG2kO0CktLY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"eul267z6U0tC"},"execution_count":null,"outputs":[]}]}